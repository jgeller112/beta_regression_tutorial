---
title: "A Beta Way: A Tutorial For Using Beta Regression in Psychological Research"
shorttitle: "Beta Regression Tutorial"
author:
  - name: Jason Geller
    corresponding: true
    orcid: 0000-0002-7459-4505
    email: drjasongeller@gmail.com
    affiliations:
      - name: Boston College
        department: Department of Psychology and Neuroscience
        address: McGuinn 300
        city: Chestnut Hill
        region: MA
        country: USA
        postal-code: 02467
    roles:
      - conceptualization
      - writing
      - editing
      - data curation
      - project administration
      - editing
      - formal analysis
      - software
      - visualization
      
  - name: Robert Kubinec
    orcid: 0000-0001-6655-4119
    affiliations:
      - name: University of South Carolina
    roles:
      - editing
      - formal analysis
      - visualization
  - name: Chelsea M. Parlett Pelleriti
    orcid: 0000-0001-9301-1398
    affiliations:
      - name: School
    roles:
      - formal analysis
      - editing
      - validation
  - name: Matti Vuorre
    orcid: 0000-0001-5052-066X
    affiliations:
      - name: Tilburg University
    roles:
      - editing
      - validation
      - formal analysis

author-note:
  status-changes:
    affiliation-change: ~
    deceased: ~
  disclosures:
    # Example: This study was registered at X (Identifier Y).
    study-registration: "No preregistration for this paper."
    # Acknowledge and cite data/materials to be shared.
    data-sharing: Data, code, and materials for this manusscript can be found at.
    related-report: ~
    conflict-of-interest: The authors have no conflicts of interest to disclose.
    financial-support: ~
    authorship-agreements: ~
abstract: "Rates, percentages, and proportions are common outcomes in psychology and the social sciences. These outcomes are often analyzed using models that assume normality, but this practice overlooks important features of the data, such as their natural bounds at 0 and 1. As a result, estimates can become distorted. In contrast, treating such outcomes as beta-distributed respects these limits and can yield more accurate estimates. Despite these advantages, the use of beta models in applied research remains limited. Our goal is to provide researchers with practical guidance for adopting beta regression models, illustrated with an example drawn from the psychological literature. We begin by introducing the beta distribution and beta regression, emphasizing key components and assumptions. Next, using data from a learning and memory study, we demonstrate how to fit a Beta regression model in R with the Bayesian package `brms` and how to interpret results on the response scale. We also discuss model extensions, including zero-inflated, zero- and one-inflated, and ordered beta models. To promote wider adoption of these methods, we provide detailed code and materials at https://github.com/jgeller112/beta_regression_tutorial."
keywords: [beta regression, beta distribution, R, tutorial, psychology, learning and memory]
floatsintext: true
# Numbered lines (.pdf and .docx only)
numbered-lines: false
# File with references
suppress-title-page: false
# Link citations to references
link-citations: true
# Masks references that appear in the masked-citations list
mask: false
word-count: true
# Language options. See https://quarto.org/docs/authoring/language.html
lang: en
language:
  citation-last-author-separator: "and"
  citation-masked-author: "Masked Citation"
  citation-masked-date: "n.d."
  citation-masked-title: "Masked Title"
  email: "Email"
  title-block-author-note: "Author Note"
  title-block-correspondence-note: "Correspondence concerning this article should be addressed to"
  title-block-role-introduction: "Author roles were classified using the Contributor Role Taxonomy (CRediT; https://credit.niso.org/) as follows:"
bibliography: bibliography.bib

format:
  apaquarto-pdf:
    documentmode: doc
  apaquarto-docx: default
  apaquarto-html: default

execute:
  echo: false
  warning: false
  message: false
  results: hide
  freeze: auto
  fig-align: "center"
  tbl-align: "center"
  code-line-numbers: true
  keep-with-next: true
  code-overflow: wrap
  fig-width: 6
  fig-asp: 0.618
  fig-pos: "h"
  tbl-pos: "h"
  fig-dpi: 300

knitr:
  opts_chunk:
    dev: "ragg_png"
---

```{r}
#| label: packages_1
#| echo: false
#| message: false
#| warning: false

library(easystats)
library(geomtextpath)
library(scales)
library(tinytable)
library(marginaleffects)
library(extraDistr)
library(brms)
library(posterior)
library(bayesplot)
library(ggdist)
library(patchwork)
library(ggrain)
library(cowplot)
library(ggokabeito)
library(ordbetareg)
library(here)
library(knitr)
library(cmdstanr)
library(webshot2)
library(performance)
library(parameters)
library(tidyverse)

options(
  scipen = 999,
  marginaleffects_posterior_center = mean,
  brms.short_summary = TRUE,
  rms.backend = Sys.getenv("BRMS_BACKEND", "cmdstanr"),
  brms.threads = as.numeric(Sys.getenv("BRMS_THREADS", 2)),
  mc.cores = as.numeric(Sys.getenv("MAX_CORES", 4))
)


set.seed(666)
```

```{r}
#| label: ggplot-theme
#| echo: false

IS_HTML <- knitr::is_html_output()
theme_publication <- function(
  base_size = ifelse(IS_HTML, 12, 14),
  base_family = "Times"
) {
  theme(
    text = element_text(
      size = base_size,
      family = base_family,
      color = "black"
    ),
    plot.title = element_text(face = "bold", hjust = 0.5, size = base_size + 2),
    plot.subtitle = element_text(hjust = 0.5, size = base_size),
    plot.caption = element_text(hjust = 0.5, size = base_size - 2),
    axis.title = element_text(face = "bold", size = base_size),
    axis.title.x = element_text(margin = margin(t = 10)),
    axis.title.y = element_text(margin = margin(r = 10)),
    axis.text = element_text(size = base_size - 1),
    axis.line = element_line(color = "black"),
    axis.ticks = element_line(color = "black"),
    panel.grid.major = element_line(color = "gray80"),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    legend.position = "bottom",
    legend.title = element_text(face = "bold", size = base_size),
    legend.text = element_text(size = base_size - 1),
    legend.background = element_blank(),
    strip.text = element_text(face = "bold", size = base_size),
    strip.background = element_rect(fill = "gray90", color = "black")
  )
}

```

```{r}
# fit models beforehand so tables and figures are in correct place.

model1 <- read_rds(here("manuscript", "models", "model_reg_bayes.rds"))
model2 <- read_rds(here("manuscript", "models", "model_beta_bayes_dis_run.rds"))
model3 <- read_rds(here("manuscript", "models", "bayes_zib_model.rds"))
model4 <- read_rds(here("manuscript", "models", "bayes_zoib_model.rds"))
model5 <- read_rds(here("manuscript", "models", "bayes_ordbeta_phi_model.rds"))
```

```{r}
#| label: data-import
#| echo: false

fluency_data <- read.csv(here("manuscript", "data", "miko_data.csv")) %>%
  rename(
    Participant = "ResponseID",
    Fluency = "Condition",
    Time = "name",
    Accuracy = "value"
  ) %>%
  mutate(
    Accuracy = Accuracy / 10,
    Fluency = ifelse(Fluency == 1, "Fluent", "Disfluent")
  ) %>%
  filter(Time == "FreeCt1AVG") %>%
  select(-X, -Time) %>%
  mutate(Participant = dense_rank(Participant)) %>%
  relocate(Accuracy, .after = last_col())
```

Many important outcomes in psychological research can be expressed as proportions or percentages: the proportion of correct responses on a test, the proportion of looks on a particular stimulus in an eye-tracking task, or the percentage of agreement with a given statement. As a practical example from educational and cognitive research, one popular way to assess learning is by looking at the proportion of correct responses on a test. To illustrate, consider a memory experiment where participants read a short passage on a specific topic. After a brief distractor task, they complete a final memory test consisting of 10 short-answer questions, each assigned a different point value (e.g., question 1 might be worth 4 points, while question 2 might be worth 1 point). The primary outcome measure could be the proportion of points earned on each question relative to the total possible points for each question.

A key question arises: **how should proportional data be analyzed?** In psychology, there is a strong reliance on linear models (e.g., *t*-tests, ANOVAs, and regressions). However, linear models assume: (1) a normally distributed (Gaussian) outcome, (2) an unbounded response scale (ranging from $-\infty$ to $-\infty$), and (3) constant variance (homoscedasticity). These assumptions are rarely met in psychological data [@sladekova2024unicorns]. They are especially problematic for proportional data, which are naturally bounded between 0 and 1 and often exhibit heteroscedasticity—non-constant variance—particularly near the boundaries [@smithson2006; @ferrari2004; @paolino2001]. Violating these assumptions can lead to biased estimates and misleading inferences.

Another option extends the linear model framework to accommodate non-normal outcome distributions. For example, binomial or Bernoulli models (commonly referred to as logistic regression when a logit link function is used) are well-suited for binary outcomes or counts of successes out of a fixed number of trials. However, these models may still fall short when data exhibit over-dispersion, or cluster near 0 or 1.

The challenges of analyzing proportional data are not new [see @bartlett1936]. Fortunately, several existing approaches address the limitations of commonly used models. One such approach is Beta regression, an extension of the GLiM that employs the Beta distribution [@ferrari2004; @paolino2001]. Beta regression offers a flexible and robust solution for modeling proportional data by accounting for boundary effects and over-dispersion, making it a valuable alternative to traditional binomial models. This approach is particularly well-suited for psychological research because it can handle both the bounded nature of proportional data and the non-constant variance often encountered in these datasets.

## A Beta Way Forward

With the combination of open-source programming languages like R [@R] and their user-developed extensions, analyses such as Beta regression have become increasingly accessible. Yet, adoption of these methods--particularly in psychology--remains limited. One reason may be the lack of informative examples that directly apply to psychological research. Although recent years have seen a surge of interest in Beta regression [@smithson2006; @heiss2021; @vuorre2019; @bendixen2023; @coretta2025bayesian], its adoption in psychology remains limited.

While previous tutorials have discussed Beta regression, most have been limited in scope--focusing either on the basic model or offering only brief mentions of more applicable alternatives. This tutorial aims to fill that gap by offering a comprehensive and practical tutorial of Beta regression and its extensions. In addition to covering the standard Beta model, we walk through its extensions such as zero-inflated, zero-one-inflated, and ordered Beta regression. These models are important for researchers dealing with boundary values (e.g., exact 0s or 1s) or ordinal response structures.

Beyond model specification, we place strong emphasis on interpreting results on the response scale--that is, in terms of probabilities and proportions--rather than relying on often difficult to interpret parameters. This focus makes the models more accessible and meaningful for psychological applications, where effects are often easier to communicate when framed on the original scale of the outcome (e.g., changes in recall accuracy or task performance). Throughout, we provide reproducible code and annotated examples to help readers implement and interpret these models in their own work.

We begin the tutorial with a non-technical overview of the Beta distribution and its core parameters. We then walk through the process of estimating Beta regression models using the R package `brms` [@brms], illustrating each step with applied examples. To guide interpretation, we emphasize coefficients, predicted probabilities, and marginal effects calculated using the `marginaleffects` package [@margeffects2024]. We also introduce several useful extensions--zero-inflated (ZIB), zero-one-inflated (ZOIB), and ordered Beta regression--that enable researchers to model outcomes that include boundary values. Finally, all code and materials used in this tutorial are fully reproducible and available via our GitHub repository: <https://github.com/jgeller112/beta_regression_tutorial>[^1].

[^1]: In this article, we try to limit code where possible; however, the online version has all the code needed to reproduce all analyses herein. Furthermore, to promote transparency and reproduciblity, the tutorial was written in R version 4.4.3 (@R) using Quarto (v.1.5.54), an open-source publishing system that allows for dynamic and static documents. This allows figures, tables, and text to be programmatically included directly in the manuscript, ensuring that all results are seamlessly integrated into the document. In addition, we use the `rix` [@rix] R package which harnesses the power of the `nix` [@nix] ecosystem to to help with computational reproducibility. Not only does this give us a snapshot of the packages used to create the current manuscript, but it also takes a snapshot of system dependencies used at run-time. This way reproducers can easily re-use the exact same environment by installing the `nix` package manager and using the included default.nix file to set up the right environment. The README file in the GitHub repository contains detailed information on how to set this up to reproduce the contents of the current manuscript, including a video.

## Beta Distribution

Proportional data pose some challenges for standard modeling approaches: The data are bounded between 0 and 1 and often exhibit non-constant variance (heteroscedasticity) [@ferrari2004; @paolino2001]. Common distributions used within the GLM or GLiM frameworks often fail to capture these properties adequately, which can necessitate alternative modeling strategies.

Typically, the expected value (or mean) of the response variable, or changes therein, is the central estimand. The model specifies how this expected value depends on explanatory variables through two main components: a linear predictor, which combines the explanatory variables in a linear form, and a link function, which connects the expected value of the response variable to the linear predictor. In addition, a random component specifies the distribution of the response variable around its expected value (such as Poisson or binomial distributions, which belong to the exponential family) [@nelder1972]. Together, these components provide a flexible framework for modeling data with different distributional properties.

The Beta distribution is continuous and restricted to values between 0 and 1 (exclusive). Its two parameters--commonly called shape1 ($\alpha$) and shape2 ($\beta$)--govern the distribution’s location, skewness, and spread. By adjusting these parameters, the distribution can take many functional forms (e.g., it can be symmetric, skewed, U-shaped, or even approximately uniform; see @fig-curves).

To illustrate, consider a test question worth seven points. Suppose a participant scores five out of seven. The number of points received (5) can be treated as $\alpha$, and the number of points missed (2) as $\beta$. The resulting Beta distribution would be skewed toward higher values, reflecting a high performance (yellow line in @fig-curves; "Beta(5, 2)"). Reversing these values would produce a distribution skewed toward lower values, representing poorer performance (green line in @fig-curves; "Beta(2, 5)").

```{r}
#| label: fig-curves
#| fig-cap: "Beta distributions with different shape1 and shape2 parameters."
#| fig-asp: 0.52

# Function to generate Beta distribution data
make_beta_df <- function(shape1, shape2, n = 9000) {
  x <- seq(0.001, 0.999, length.out = n)
  data.frame(
    x = x,
    y = dbeta(x, shape1, shape2),
    label = paste0("Beta(", shape1, ", ", shape2, ")")
  )
}

# Combine distributions
beta_df <- bind_rows(
  make_beta_df(0.5, 0.5),
  make_beta_df(2, 2),
  make_beta_df(5, 2),
  make_beta_df(2, 5)
) %>%
  mutate(label = factor(label))

ggplot(
  beta_df,
  aes(x = x, y = y, label = label, group = label, color = label)
) +
  geom_line(linewidth = 0.9) +
  scale_color_okabe_ito(
    name = "Distribution"
  ) +
  scale_x_continuous(
    "Proportion",
    expand = expansion(0.01)
  ) +
  scale_y_continuous(
    "Density",
    limits = c(0, 5),
    expand = expansion(0.01)
  ) +
  theme_publication() +
  theme(legend.position = "right")
```

## Beta Regression

While the standard parameterization of the Beta distribution uses $\alpha$ and $\beta$, a reparameterization to a mean ($\mu$) and precision ($\phi$) is more useful for regression models. The mean represents the expected value of the distribution, while the dispersion, which is inversely related to variance, reflects how concentrated the distribution is around the mean, with higher values indicating a narrower distribution and lower values indicating a wider one. The connections between the Beta distribution's parameters are shown in @eq-beta. Importantly, the variance depends on the average value of the response.

$$
\begin{aligned}[t]
\text{Shape 1:} && a &= \mu \phi \\
\text{Shape 2:} && b &= (1 - \mu) \phi
\end{aligned}
\qquad\qquad\qquad
\begin{aligned}[t]
\text{Mean:} && \mu &= \frac{a}{a + b} \\
\text{Precision:} && \phi &= a + b \\
\text{Variance:} && var &= \frac{\mu \cdot (1 - \mu)}{1 + \phi}
\end{aligned}
$$ {#eq-beta}

Thus, Beta regression allows modeling both the mean and precision of the outcome distribution. To ensure that $\mu$ stays between 0 and 1, we apply a link function, which allows linear modeling of the mean on an unbounded scale. A common link-function choice is the logit, but other functions such as the probit or complementary log-log are possible.

The logit function, $\text{logit}(\mu) = \log \left( \frac{\mu}{1 - \mu} \right)$ links the mean to log-odds which are unbounded, making linear modeling possible. The inverse of the logit, called the logistic function, maps the linear predictor $\eta$ back to the original scale of the data $\left(\mu = \frac{1}{1 + e^{-\eta}}\right)$. Similarly, the strictly positive dispersion parameter is usually modeled through a log link function, ensuring it remains positive.

By accounting for the observations' natural limits and non-constant variance across different values, the Beta distribution is useful in psychology where outcomes like performance rates or response scales frequently exhibit these features.

## Bayesian Approach to Beta Regression

Beta regression models can be estimated with both frequentist and Bayesian methods. We adopt a Bayesian framework because it makes estimating and interpreting more complex models easier [@gelman2013bayesian; @mcelreath2020statistical; @bayes_rules]. We use the R package `brms` [@brms], a high-level interface to the probabilistic programming language Stan [@stan2023], because it uses standard R regression formula syntax but extends its scope while remaining accessible for non-expert users.

There are several important differences between our Bayesian analysis and the frequentist methods readers may be more familiar with—most notably, the absence of *t*- and *p*-values. To estimate models, the `brms` package uses Stan’s computational algorithms to draw random samples from the posterior distribution, which represents uncertainty about the model parameters. This posterior is conceptually analogous to a frequentist sampling distribution.

By default, the Bayesian models run 4,000 posterior draws, which allow us to compute quantities such as the posterior mean (similar to a frequentist point estimate) and the 95% credible interval (Cr.I), which is often compared to a confidence interval. In addition, we report the *probability of direction* (pd), which reflects the probability that a parameter is strictly positive or negative. A pd of 95%, 97.5%, 99.5%, and 99.95% corresponds approximately to two-sided *p*-values of 0.10, 0.05, 0.01, and 0.001, respectively. For directional hypotheses, the pd can be interpreted as roughly equivalent to one minus the *p*-value [@marsmanThreeInsightsBayesian2016].

For reasons of space, we refer readers unfamiliar to Bayesian data analysis to several existing books on the topic [@gelman2013bayesian; @mcelreath2020statistical; @kruschke2015doing]. In addition, we assume readers are familiar with R, but those in need of a refresher should find @wickhamDataScienceImport2023 useful.

# Beta Regression Tutorial

## Example Data

Throughout this tutorial, we analyze data from a memory experiment examining whether the fluency of an instructor’s delivery affects recall performance [@wilford2020, Experiment 1A]. Instructor fluency—marked by expressive gestures, dynamic vocal tone, and confident pacing—has been shown to influence students’ perceptions of learning, often leading learners to rate fluent instructors more favorably [@carpenter2013]. However, previous research suggests that these impressions do not reliably translate into improved memory performance [e.g., @carpenter2013; @toftness2017; @witherby2022]. In contrast, @wilford2020 found that participants actually recalled more information after watching a fluent instructor compared to a disfluent one. This surprising finding makes the dataset a compelling case study for analyzing proportion data, as recall was scored out of 10 possible idea units per video.

In Experiment 1A, participants watched two short instructional videos, each delivered either fluently or disfluently. Fluent videos featured instructors with smooth delivery and natural pacing, while disfluent videos included hesitations, monotone speech, and awkward pauses. After a distractor task, participants completed a free recall test, writing down as much content as they could remember from each video within a three-minute window. Their recall was then scored for the number of idea units correctly remembered.

```{r}
#| lst-label: lst-data
#| lst-cap: Data needed to run examples
#| eval: false
#| echo: true

# get data here from github
fluency_data <- read_csv(
  "https://raw.githubusercontent.com/jgeller112/beta_regression_tutorial/refs/heads/main/manuscript/data/fluency_data.csv"
)
```

Our primary outcome variable is the proportion of idea units recalled on the final test, calculated by dividing the number of correct units by 10. We show a sample of these data in @tbl-dataset. The dataset can be downloaded from Github (@lst-data). Because this is a bounded continuous variable (i.e., it ranges from 0 to 1), it violates the assumptions of typical linear regression models that treat outcomes as normally distributed. Despite this, it remains common in psychological research to analyze proportion data using models that assume normality. In what follows, we reproduce Wilford et al.'s analysis and then re-analyze the data using Beta regression and highlight how it can improve our inferences.

```{r}
#| label: tbl-dataset
#| tbl-cap: "Four observations from @wilford2020. Accuracy refers to the proportion of correctly recalled idea units."
#| echo: false
#| after-caption-space: 0pt

fluency_data |>
  sample_n(4) |>
  tt() |>
  format_tt(escape = TRUE)
```

## Reanalysis of Wilford et al. Experiment 1A

In their original analysis of Experiment 1A, @wilford2020 compared memory performance between fluent and disfluent instructor conditions using a traditional independent-samples t-test. They found that participants who watched the fluent instructor recalled significantly more idea units than those who viewed the disfluent version (see @fig-flu1).

```{r}
#| label: fig-flu1
#| fig-cap: "Raincloud plot depicting accuracy distributions for the Fluent and Disfluent conditions. Each condition shows individual data points, a density plot, and summary statistics to illustrate variability and central tendency."

ggplot(fluency_data, aes(x = Fluency, y = Accuracy, fill = Fluency)) +
  geom_rain(
    aes(color = Fluency),
    shape = 21,
    size = 1.5,
    alpha = 0.6,
    width = 0.1
  ) +
  # Add black mean dot, nudged to center
  stat_summary(
    fun = mean,
    geom = "point",
    shape = 21,
    size = 3,
    fill = "black",
    color = "black",
    position = position_nudge(x = .1) # adjust as needed
  ) +
  scale_y_continuous(labels = label_percent()) +
  scale_fill_okabe_ito(aesthetics = c("fill", "color")) +
  theme_publication() +
  theme(
    legend.position = "none",
    axis.title.x = element_blank()
  )


```

We first replicate this analysis in a regression framework using brms. We model final test accuracy--the proprtion of correctly recalled idea units across the two items--as the dependent variable. Our predictor is instructor fluency, with two levels: Fluent and Disfluent. We use treatment (dummy) coding, which is the default in R. This coding scheme sets the first level of a factor (in alphabetical order) as the reference level. In this case, Disfluent is the reference, and the coefficient for Fluent reflects the contrast between fluent and disfluent instructor conditions.

```{r}
#| label: tbl-all
#| tbl-cap: "Bayesian Regression summaries for each model"
#| tbl-align: "center"
#| apa-note: "Reported as: Mean \\[95% CrI\\] (pd). Link functions:  b_mean = logit; b_phi = logit; b_zoi (zero-one inflation) = logit; b_coi (conditional one-inflation) = logit."

include_graphics(here(
  "manuscript",
  "Figures",
  "combined_table.png"
))

```

### Regression Model

We first start by loading the `brms` [@brms] and `cmdstanr` [@cmdstanr] packages (@lst-loadbrms). We use the `cmdstanr` backend for Stan [@stan2023] because it's faster and more modern than the default used to run models (i.e., `rstan`).[^2]

[^2]: In order to use the `cmdstanr` backend you will need to first install the package and also run `cmdstanr::install_cmdstan()` if you have not done so already.

```{r}
#| lst-label: lst-loadbrms
#| lst-cap: Load the `brms` and `cmdstanr`
#| echo: true
#| eval: false

library(brms)
library(cmdstanr)
```

```{r}
#| label: fit-gauss
#| lst-label: lst-gauss
#| lst-cap: Fitting a gaussian model with brm().
#| collapse: true
#| hide: false
#| echo: true

bayes_reg_model <- brm(
  Accuracy ~ Fluency,
  data = fluency_data,
  family = gaussian(),
  file = "model_reg_bayes"
)

```

```{r}
#| label: fit-gauss-parameters
#| echo: false

brms_model_new <- model_parameters(bayes_reg_model, centrality = "mean")

```

We fit the model using the `brm()` function from the `brms` package (@lst-gauss). Although not shown here, we ran the models using four chains (the default), executed in parallel across four cores. When you run the model in @lst-gauss, the output below will appear in your console. The output from `bayes_reg_model` shows each parameter's posterior summary: The posterior distribution's mean and standard deviation (analogous to the frequentist standard error) and its 95% Cr.I, which indicate the 95% of the most credible parameter values. Additionally, the output indicates numerical estimates of the sampling algorithm's performance: Rhat should be close to one, and the ESS (effective sample size) metrics should be as large as possible given the number of iterations specified (default is 4000). Generally, ESS \>= 1000 is recommended [@brms].

Looking at the output, the `Intercept` refers to the mean accuracy in the *disfluent* condition, as fluency was dummy-coded. The fluency coefficient (`FluencyFluent`) reflects the mean difference in recall accuracy between the fluent and disfluent conditions: *b* = `r round(brms_model_new$Mean[2], 3)`. The 95% Cr.I for this estimate spans from `r round(brms_model_new$CI_low[2], 3)` to `r round(brms_model_new$CI_high[2], 3)`. These values are shown in the "95% Cr.I" columns of the output. These results closely mirror the findings reported by @wilford2020: *t*(94) = 2.00, *p* = .048, 95% CI \[0.06, 16.56\].

The output also includes the ESS and Rhat values, both of which are within acceptable ranges, indicating good model convergence. Throughout the tutorial, we focus on the posterior mean estimates and their 95% credible intervals. In addition, we also include pd measure in the main summary table (@tbl-all) which are provided by the `easystats` `bayestestr` package [@makowski2019a' @makowski2019] and provides a parallel to *p*-values most readers are probably familiar with.

```{r}
#| label: show reg model
summary(bayes_reg_model)
```

## Beta Regression

@wilford2020 observed that instructor fluency impacts actual learning, using a t-test. But recall this approach assumes normality of residuals and homoscedacity. These assumptions are unrealistic when the response values approach the scale boundaries [@sladekova2024unicorns]. Does the data we have meet those assumptions? We can use the function `check_model` from `easystats` [@easystats] to check our assumptions easily. The code in @lst-check1 automatically produces @fig-reg-assump. We can see some issues with our data. Specifically, there appears to be violations of normality constant variance (homogeneity).

```{r}
#| label: fig-reg-assump
#| lst-label: lst-check1
#| lst-cap: Checking assumptions with the `check_model()` from `easystats` package .
#| fig-cap: "Two assumption checks for our OLS model: Normality (left) and Homogeneity (right)"
#| echo: true

check_model(bayes_reg_model, check = c("homogeneity", "normality"))
```

We can also examine how well the data fits the model by performing a posterior predictive check using the `pp_check()` function from `brms`. A posterior predictive check involves looking at multiple draws or repetitions from the posterior distribution and plotting it against the observed data. Ideally, the predictive draws (the light blue lines) should show reasonable resemblance with the observed data (dark blue line). In our example (see @fig-ppcheckall (A)) the model-predicted density is slightly too peaked and narrow compared to the data. In addition, some of the draws extend into negative accuracy values.

```{r}
#| label: fig-ppcheckall
#| fig-cap: Posterior predictive checks for regular regression (A), Beta regression (B), and ZIB (C) models
#| fig-height: 8
#| fig-width: 8

#pp_cbeck

pp_check1 <- pp_check(model1, ndraws = 100) +
  theme_publication()

pp_check2 <- pp_check(model2, ndraws = 100) +
  theme_publication()

pp_check3 <- pp_check(model3, ndraws = 100) +
  theme_publication()


# Use `plot_spacer()` for the empty 4th spot
combined_plot <- (pp_check1 + pp_check2 + pp_check3) +
  plot_annotation(
    tag_levels = 'A',
    tag_prefix = "(",
    tag_suffix = ")"
  ) +
  labs(
    caption = "Light blue lines are model-predicted data
                 dark blue line is observed data"
  )


combined_plot
```

Given the outcome variable is proportional, one solution would be to run a beta regression model. Again, we can create the beta regression model in `brms`. In `brms`, we model each parameter independently. Recall from the introduction that in a Beta model we model two parameters--$\mu$ and $\phi$. We can easily do this by using the `bf()` function from `brms` (@lst-beta01). `bf()` facilitates the specification of several sub-models within the same formula call. We fit two formulas, one for $\mu$ and one for $\phi$ and store it in the `model_beta_bayes` object below. In the below `bf()` call, we are modeling Fluency as a function of Accuracy only for the $\mu$ parameter. For the $\phi$ parameter, we are only modeling the intercept value. This is saying dispersion does not change as a function of fluency.

If we try to run a model with our data `data_fleuncy` we get an error: `Error: Family 'beta' requires response greater than 0`. This is because the Beta distribution only supports observations in the 0 to 1 interval *excluding exact zeros and ones*. We need make sure there are no zeros and ones in our dataset.

If we look at the dataset we will see that it contains nine zeros and one 1. To run a Beta regression, a common hack is to nudge our 0s towards .01 and our 1s to .99 so they fall within the \[0, 1\] interval. The model from @lst-beta01 uses a transformed `data_fluency` object (called `data_beta`) where 0s and 1s are changed to .01 and .99, respectively. When we run it we should not get no error.

```{r}
#| label: beta_brms_01s
#| lst-label: lst-beta01
#| lst-cap: Fitting a beta model without 0s and 1s in brm().
#| echo: true

# set up model formual
model_beta_bayes <- bf(
  Accuracy ~ Fluency, # fit mu model
  phi ~ 1 # fit phi model
)


# transform 0 to 0.1 and 1 to .99
data_beta <- fluency_data |>
  mutate(
    Accuracy = ifelse(Accuracy == 0, .01, ifelse(Accuracy == 1, .99, Accuracy))
  )

beta_brms <- brm(
  model_beta_bayes,
  data = data_beta,
  family = Beta(),
  file = "model_beta_bayes_reg_1"
)
```

### Model Parameters

In @tbl-all under the Beta Regression column, the first set of coefficients represent how factors influence the $\mu$ parameter estimates (which is the mean of the Beta distribution), which are labeled with an underscore `b_`. These coefficients are interpreted on the scale of the logit, meaning they represent linear changes on a nonlinear space. The intercept term `(b_Intercept)` represents the log odds of the mean on accuracy for the fluent instructor. Log odds that are negative indicate that it is more likely a "success" (like getting the correct answer) will NOT happen than that it will happen. Similarly, regression coefficients in log odds forms that are negative indicate that an increase in that predictor leads to a decrease in the predicted probability of a "success".

The other component we need to pay attention to is the dispersion or precision parameter coefficients labeled as `b_phi` in @tbl-all. The dispersion ($\phi$) parameter tells us how precise our estimate is. Specifically, $\phi$ in Beta regression tells us about the variability of the response variable around its mean. Specifically, a higher dispersion parameter indicates a narrower distribution, reflecting less variability. Conversely, a lower dispersion parameter suggests a wider distribution, reflecting greater variability. The main difference between a dispersion parameter and the variance is that the dispersion has a different interpretation depending on the value of the outcome, as we show below. The best way to understand dispersion is to examine visual changes in the distribution as the dispersion increases or decreases.

Understanding the dispersion parameter helps us gauge the precision of our predictions and the consistency of the response variable. In `beta_brms` we only modeled the dispersion of the intercept. When $\phi$ is not specified, the intercept is modeled by default (see @tbl-all). The intercept under the precision heading is not that interesting. It represents the overall dispersion in the outcome across all conditions. Instead, we can model different dispersions across levels of the `Fluency` factor. To do so, we add `Fluency` to the phi model in `bf()`. We model the precision (`phi`) of the `Fluency` factor by using a `~` and adding factors of interest to the right of it (@lst-betadis).

```{r}
#| label: brms-model-dis
#| lst-label: lst-betadis
#| lst-cap: Fitting Beta model with dispersion in `brm()`.
#| results: hide
#| echo: true

model_beta_bayes_disp <- bf(
  Accuracy ~ Fluency, # Model of the mean
  phi ~ Fluency # Model of the precision
)

beta_brms_dis <- brm(
  model_beta_bayes_disp,
  data = data_beta,
  family = Beta(),
  file = "model_beta_bayes_dis_run"
)

```

@tbl-all displays the model summary with the precision parameter added to our model as a function of fluency. It is important to note that the estimates are logged and not on the original scale (this is only the case when additional parameters are modeled). To interpret them on the original scale, we can exponentiate the log-transformed value--this transformation gets us back to our original scale. In the below model call, we set `exponentiate` = TRUE.

```{r}
#| label: phi-beta
#| echo: true
#| after-caption-space: 0pt

beta_model_dis_exp <- beta_brms_dis |>
  model_parameters(exponentiate = TRUE, centrality = "mean")
```

```{r}
#| label: tbl-phi-beta-exp
#| echo: false
#| tbl-cap: Beta regression model summary for fluency factor with $\phi$ parameter exponentiated
#| after-caption-space: 0pt

beta_model_dis_exp |>
  filter(startsWith(Parameter, "b_phi")) |>
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))),
    `95% CI` = str_glue("[{CI_low}, {CI_high}]")
  ) |>
  select(Parameter, Mean, `95% CI`, pd) |>
  tt(digits = 3) |>
  format_tt(escape = TRUE)

```

The $\phi$ intercept represents the precision of the fluent condition. The $\phi$ coefficient for `FluencyFluent` represents the change in that precision for performance between the fluent vs. disfluent conditions. The Cr.I does not include 0: Zero is not among the 95% most credible parameter values.

It is important to note that these estimates are not the same as the marginal effects we discussed earlier. Changes in dispersion affect the spread or variability of the response distribution without necessarily altering its mean. This makes dispersion particularly relevant for research questions that focus on features of the distribution beyond the average—such as how concentrated responses are. For instance, high dispersion might indicate that individuals cluster at the extremes (e.g., very high or very low ratings), suggesting clustering in the outcome.

A critical assumption of the GLM is homoscedasticity, which means constant variance of the errors. Here we see one of the benefits of a beta regression model: we can include a dispersion parameter for Fluency. Properly accounting for dispersion is crucial because it impacts the precision of our mean estimates and, consequently, the significance of our coefficients. The inclusion of dispersion in the our model increased the uncertainty of the $\mu$ coefficient (see @fig-plt-diff)--the Cr.I now includes zero. This suggests that failing to account for the dispersion of the variables might lead to biased estimates. This highlights the potential utility of an approach like Beta regression over a traditional approach as Beta regression can explicitly model dispersion and address issues of heteroscedasticity.

We won't always need to include dispersion parameters for each of our variables. One approach would be to compare models, for example with leave one out (loo) cross validation, to examine if a dispersion parameter should be considered in our model.

```{r}
#| label: fig-plt-diff
#| fig-cap: "Comparison of posterior distributions for the risk difference in fluency: Simple model  (no dispersion for Fluency) vs. complex model with  dispersion"
#| echo: false

# Add a model identifier to each dataset
pred_no <- avg_comparisons(beta_brms, variables = "Fluency") |>
  posterior_draws() |>
  mutate(model = "No Dispersion")

pred_yes <- avg_comparisons(beta_brms_dis, variables = "Fluency") |>
  posterior_draws() |>
  mutate(model = "Dispersion")

# Combine both datasets
pred_combined <- bind_rows(pred_no, pred_yes)

ggplot(pred_combined, aes(x = draw, y = model, fill = model)) +
  stat_halfeye(.width = c(.8, .95), alpha = 0.7) +
  scale_fill_okabe_ito() +
  # guides(fill = guide_legend(title = "Model")) +  # Remove or comment out
  labs(
    x = "Fluency",
    y = "Difference in Final Recall Performance",
    caption = "80% and 95% credible intervals shown in black"
  ) +
  geom_vline(xintercept = 0, color = "black", linetype = "solid", size = 1.2) +
  theme_publication() +
  theme(legend.position = "none")
```

```{r}
#| label: beta-brms
#| echo: false
#| eval: false
#| after-caption-space: 0pt

model_parameters(beta_brms, "mean") |>
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))),
    `95% Cr.I` = str_glue("[{CI_low}, {CI_high}]")
  ) |>
  select(Parameter, Mean, `95% CI`, pd) |>
  tt() |>
  format_tt(digits = 3) |>
  format_tt(escape = TRUE)
```

```{r}
#| label: beta-mu
#| eval: false
#| echo: false
#| after-caption-space: 0pt

model_parameters(beta_brms, "mean") |>
  filter(!startsWith(Parameter, "b_phi")) |>
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))),
    `95% CI` = str_glue("[{CI_low}, {CI_high}]")
  ) |>
  select(Parameter, Mean, `95% CI`, pd) |>
  tt() |>
  format_tt(digits = 3) |>
  format_tt(escape = TRUE)

```

### Predicted Probabilities

Parameter estimates are usually difficult to intercept on their own. We argue that researchers should not spend too much time interpreting single model estimates. We report them in this tutorial for completeness. Instead researchers should discuss the effects of the predictor on the actual outcome of interest (in this case the 0-1 scale). The logit link allows us to transform back and forth between the scale of a linear model and the nonlinear scale of the outcome, which is bounded by 0 and 1. By using the inverse of the logit, we can easily transform our linear coefficients to obtain average effects on the scale of the proportions or percentages, which is usually what is interesting to applied researchers. In a simple case, we can do this manually, but when there are many factors in your model this can be quite complex.

In our example, we can use the `plogis()` function in base R to convert estimates from the log-odds (logit) scale to the probability scale. The intercept of our model is -0.918, which reflects the log-odds of the mean accuracy in the disfluent condition. If the estimated difference between the fluent and disfluent conditions is 0.24 on the log-odds scale, we first add this value to the intercept value (-0.918) to get the log-odds for the fluent condition: `-0.918 + 0.24 = -0.678`. We then use `plogis()` to convert both log-odds values to probabilities (Fluent = 36%, Disfluency = 26%).

This is pretty easy to do manually, but when your model has many predictors, it can be quite cumbersome. To help us extract predictions from our model and visualize them we will use a package called **`marginaleffects`** [@margeffects2024] (see @lst-me). To get the proportions for each of our categorical predictors on the mu parameter we can use the function from the package called `predictions()`. These are displayed in @tbl-predict-prob. These probablities match what we calculated above.

```{r}
#| echo: true
#| lst-label: lst-me
#| lst-cap: Load the `marginaleffects` package.

library(marginaleffects)
```

```{r}
#| lst-label: lst-pred
#| lst-cap: Predictions from the beta model for each level of Fluency.
#| eval: false
#| echo: true

predictions(
  beta_brms,
  # need to specify the levels of the categorical predictor
  newdata = datagrid(Fluency = c("Disfluent", "Fluent"))
)
```

```{r}
#| label: tbl-predict-prob
#| tbl-cap: Predicted probablities for fluency factor.
#| tbl-cap-location: top
#| echo: false
#| after-caption-space: 0pt

predictions(
  beta_brms,
  newdata = datagrid(Fluency = c("Disfluent", "Fluent"))
) |>
  mutate(Fluency = ifelse(Fluency == "Fluent", "Fluent", "Disfluent")) |>
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))),
    `95% CI` = str_glue("[{conf.low}, {conf.high}]")
  ) |>
  rename("Mean" = "estimate") |>
  select(Fluency, Mean, `95% CI`) |>
  relocate(Fluency) |>
  tt(digits = 3) |>
  format_tt(escape = TRUE)
```

For the `Fluency` factor, we can interpret `Mean` as proportions or percentages. That is, participants who watched the fluent instructor scored on average 36% on the final exam compared to 26% for those who watched the disfluent instructor. We can also visualize these from `marginaleffects` using the `plot_predictions()` function (see @lst-plotbeta).

```{r}
#| label: plot-beta
#| lst-label: lst-plotbeta
#| lst-cap: Plot predicted probablities using `plot_predictions()` from `marginaleffects`
#| echo: true

beta_plot <- plot_predictions(beta_brms, condition = "Fluency")
```

The `plot_predictions()` fucntuon will only display the point estimate with the 95% Cr.Is. However, Bayesian estimation methods generate distributions for each parameter. This approach allows visualizing full uncertainty estimates beyond points and intervals. Using the `marginaleffects` package, we can obtain samples from the posterior distribution with the `posterior_draws()` function (see @lst-posdraws). We can then plot these results to illustrate the range of plausible values for our estimates at different levels of uncertainty (e.g., 80% or 95%; see @fig-draws-fluency).

```{r}
#| lst-label: lst-posdraws
#| lst-cap: Extracting posterior draws from the beta regression model.
#| echo: true

# Add a model identifier to each dataset
pred_draws_beta <- avg_predictions(beta_brms, variables = "Fluency") |>
  posterior_draws()
```

```{r}
#| label: fig-draws-fluency
#| fig-cap: Predicted probablity posterior distributions by fluency
# Plot both models
ggplot(pred_draws_beta, aes(x = draw, y = Fluency, fill = Fluency)) +
  stat_halfeye(.width = c(.5, .8, .95), alpha = 0.7) +
  scale_fill_okabe_ito() +
  scale_x_continuous(labels = label_percent()) +
  labs(
    x = "Accuracy",
    y = "Fluency",
    caption = "50%, 80% and 95% credible intervals shown in black"
  ) +
  geom_vline(xintercept = 0, color = "black", linetype = "solid", size = 1.2) +
  theme_publication() +
  theme(legend.position = "none")
```

### Marginal Effects

Marginal effects provide a way to understand how changes in a predictor influence an outcome, holding all other factors constant in a specific manner. Technically, marginal effects are calculated using partial derivatives for continuous variables or finite differences for categorical and continuous variables, depending on the nature of the data and the research question. Substantively, these effects translate regression coefficients into a form that can be interpreted directly on the outcome scale of interest.

There are various types of marginal effects, and their calculation can vary across software packages. For example, the popular `emmeans` package [@emmeans]computes marginal effects by holding all predictors at their means. In this tutorial, we will use the `marginaleffects` package [@margeffects2024], which focuses on average marginal effects (AMEs) by default. AMEs summarize effects by generating predictions for each row of the original dataset and then averaging these predictions. This approach retains a strong connection to the original data while offering a straightforward summary of the effect of interest.

One practical application of AMEs is calculating the average difference between two groups or conditions (called the risk difference). Using the `avg_comparisons()` function in the `marginaleffects` package (@lst-avgcomp), we can compute this metric directly. By default, the function calculates the discrete difference between groups. The function can also compute other effect size metrics, such as odds ratios and risk ratios, depending on the research question. This flexibility makes it a powerful tool for interpreting regression results in a meaningful way.

```{r}
#| label: ame1
#| lst-label: lst-avgcomp
#| lst-cap: "Calculating the difference between probablities with `avg_comparisons()`"
#| echo: true
#|
# get risk difference by default

beta_avg_comp <- avg_comparisons(beta_brms, comparison = "difference")
```

```{r}
#| label: tbl-ame1
#| echo: false
#| tbl-cap: Probability fluency difference
#| after-caption-space: 0pt

beta_avg_comp |>
  # remove unwanted variables
  select(-predicted_lo, -predicted_hi, -tmp_idx, -predicted) |>
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))),
    `95% CI` = str_glue("[{conf.low}, {conf.high}]")
  ) |>
  rename("Contrast" = "contrast", "Mean" = "estimate", "Term" = "term") |>
  select(Term, Contrast, Mean, `95% CI`) |>
  tt(digits = 2) |>
  format_tt(escape = TRUE)
```

@tbl-ame1 displays the difference for the fluency factor (`Mean` column). The difference between the fluent and disfluent conditions is .09. That is, participants who watched a fluent instructor scored 9% higher on the final recall test than participants who watched the disfluent instructor. Our Cr.I \[-.0174, -0.011\] shows that zero is not among the 95% most credible values.

```{r}
#| label: tbl-or
#| echo: false
#| tbl-cap: Odds ratio for fluency factor
#| after-caption-space: 0pt

# get odds ratios as an example
avg_comparisons(beta_brms, comparison = "lnoravg", transform = "exp") |>
  select(-predicted_lo, -predicted_hi, -tmp_idx, -predicted) |>
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))),
    `95% CI` = str_glue("[{conf.low}, {conf.high}]")
  ) |>
  rename("Contrast" = "contrast", "Mean" = "estimate", "Term" = "term") |>
  select(Term, Contrast, Mean, `95% CI`) |>
  tt() |>
  format_tt(escape = TRUE)
```

```{r}
# Proportions
p1 <- 0.37 # Fluent
p2 <- 0.29 # Disfluent

# Cohen's h formula
cohens_h <- 2 * (asin(sqrt(p1)) - asin(sqrt(p2)))


#CI lower and upper bounds
p1_lower <- 0.328
p1_upper <- 0.419
p2_lower <- 0.250
p2_upper <- 0.329

# Lower bound of h
h_lower <- 2 * (asin(sqrt(p1_lower)) - asin(sqrt(p2_upper)))
# Upper bound of h
h_upper <- 2 * (asin(sqrt(p1_upper)) - asin(sqrt(p2_lower)))

```

In psychology, it is common to report effect size measures like Cohen's *d* [@cohen1977]. When working with proportions we can calculate something similar called Cohen's *h*. Taking our proportions, we can use the below equation to calculate Cohen's *h* along with the 95% CIs around it. Using this metric we see the effect size is small (`r round(cohens_h, 3)`), 95% Cr.I \[`r round(h_lower,3)`, `r round(h_upper,3)`\].

$$
h = 2 \cdot \left( \arcsin\left(\sqrt{p_1}\right) - \arcsin\left(\sqrt{p_2}\right) \right)
$$

### Posterior Predictive Check

@fig-ppcheckall (B) shows the predictive check for our beta model. The model does a pretty good job at capturing the data (The draws are now between 0-1) and the the model predicted values follow the observed data. However, it could be better.

```{r}
#| label: fig post pred
pp_check2 <- pp_check(beta_brms_dis, ndraws = 100) +
  theme_publication()
```

## Zero-Inflated Beta (ZIB) Regression

A limitation of the Beta regression model is that it can only accommodate values strictly between 0 and 1—it cannot handle values exactly equal to 0 or 1. In our dataset, we observed 9 rows where Accuracy equals zero. To fit a Beta model, we nudged these values to 0.01, but this kind of data manipulation is generally discouraged, especially when the zeros are meaningful. In our case, these zeros may be structural—that is, they represent real, systematic instances where participants failed to answer correctly (rather than random noise or measurement error). For example, the fluency of the instructor might be a key factor in predicting these zero responses. To properly account for them, we can use a zero-inflated Beta (ZIB) model. This model still estimates the mean ($\mu$) and precision ($\phi$) of the Beta distribution for values between 0 and 1, but it also includes an additional parameter, $\alpha$, which captures the probability of observing structural zeros.

The zero-inflated Beta models a mixture of the data-generating process. The $\alpha$ parameter uses a logistic regression to model whether the data is 0 or not. Substantively, this could be a useful model when we think that 0s come from a process that is relatively distinct from the data that is greater than 0. For example, if we had a dataset of with proportion of looks or eye fixations to certain areas on marketing materials, we might want a separate model for those that do not look at certain areas on the screen because individuals who do not look might be substantively different than those that look.

We can fit a ZIB model using `brms` and use the `marginaleffects` package to make inferences about our parameters of interest. Before we run a zero-inflated beta model, we will need to transform our data again and nudge our 1s to .99--we can keep our zeros. Similar to our Beta regression model we fit in `brms`, we will use the `bf()` function to fit several models. We fit our $\mu$ and $\phi$ parameters as well as our zero-inflated parameter ($\alpha$; here labeled as `zi`). In `brms` we can use the zero_inflated_beta family (see @lst-zib).

```{r}
#| label: zib brms model
#| lst-label: lst-zib
#| lst-cap: Fitting zib model with `brm()`
#| echo: true

# keep 0 but transform 1 to .99
data_beta_0 <- fluency_data |>
  mutate(Accuracy = ifelse(Accuracy == 1, .99, Accuracy))

# set up model formual for zero-inflated beta in brm
zib_model <- bf(
  Accuracy ~ Fluency, # The mean of the 0-1 values, or mu
  phi ~ Fluency, # The precision of the 0-1 values, or phi
  zi ~ Fluency, # The zero-or-one-inflated part, or alpha
  family = zero_inflated_beta()
)

# fit zib model with brm
fit_zi <- brm(
  formula = zib_model,
  data = data_beta_0,
  file = "bayes_zib_model"
)
```

### Posterior Predictive Check

The ZIB model does a bit better at capturing the sturture of the data then the Beta regression model (see @fig-ppcheckall). Specifically, the ZIB model more accurately captures the increased density of values near the lower end of the scale (i.e., near zero), which the standard Beta model underestimates. The ZIB model’s predictive distributions also align more closely with the observed data across the entire range, particularly in the peak and tail regions. This improved fit likely reflects the ZIB model’s ability to explicitly model excess zeros (or near-zero values) via its inflation component, allowing it to better account for features in the data that a standard Beta distribution cannot accommodate.

```{r}
#| label: fig ppcheckzib
#| fig-cap: Posterior predictive check for the zib model
pp_check3 <- pp_check(fit_zi, ndraws = 100) + # ndraws is nu
  theme_publication()
```

### Predicted Probabilities and Marginal Effects

@tbl-all under the zero-inflated Beta regression column provides a summary of the posterior distribution for each parameter. As stated before, it is preferable to back-transform our estimates to get probabilities. To get the predicted probabilities we can again use the `avg_predictions()` and `avg_comparisons()` functions from `marginaleffects` package [@marginaleffects] to get predicted probablities and the probability difference between the levels of each factor. We can model the parameters separately using the `dpar` argument setting to: $\mu$, $\phi$, $\alpha$. Here we look at the risk difference for Fluency under each parameter.

#### Mean

As shown in @tbl-marg-mu-brms, there is little evidence for an effect of Fluency -- the 95% CI includes zero, suggesting substantial uncertainty about the direction and magnitude of the effect.

```{r}

marg_zi_brms <- fit_zi |>
  avg_comparisons(
    variables = "Fluency",
    dpar = "mu",
    comparison = "difference"
  )
```

```{r}
#| label: tbl-marg-mu-brms
#| tbl-cap:  Probablity fluency difference ($\mu$)
#| echo: false
#| after-caption-space: 0p

marg_zi_brms |>
  select(term, contrast, estimate, conf.low, conf.high) |>
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))),
    `95% CI` = str_glue("[{conf.low}, {conf.high}]")
  ) |>
  rename("Contrast" = "contrast", "Mean" = "estimate", "Term" = "term") |>
  select(Term, Contrast, Mean, `95% CI`) |>
  tt() |>
  format_tt(digits = 3) |>
  format_tt(escape = TRUE)

```

#### Dispersion

As shown in @tbl-marg-phi-brms, the posterior estimates suggest a credible effect of Fluency on dispersion ($\phi$), with disfluent responses showing greater variability. The 95% CI for the fluency contrast does not include zero, indicating meaningful differences in precision.

```{r}

marg_phi_brms <- fit_zi |>
  avg_comparisons(
    variables = "Fluency",
    dpar = "phi",
    comparison = "difference"
  )
```

```{r}
#| label: tbl-marg-phi-brms
#| tbl-cap:  Probablity fluency difference ($\phi$)
#| echo: false
#| after-caption-space: 0pt

marg_phi_brms |>
  select(term, contrast, estimate, conf.low, conf.high) |>
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))),
    `95% CI` = str_glue("[{conf.low}, {conf.high}]")
  ) |>
  rename("Contrast" = "contrast", "Mean" = "estimate", "Term" = "term") |>
  select(Term, Contrast, Mean, `95% CI`) |>
  tt() |>
  format_tt(digits = 3) |>
  format_tt(escape = TRUE)

```

### Zero-Inflation

We can harness the power of `marginaleffects` again and plot the posterior difference between the fluent and disfluent conditions (see @fig-zi-plot).In @fig-zi-plot, there is evidence that watching a lecture video with a fluent instructor reduces the probability of a zero response by approximately 13%. The 95% CI for this effect does not include zero, suggesting a meaningful reduction in the likelihood of zero outcomes under fluent instruction. We can harness the power of `marginaleffects` again and plot the posterior probablity of each level (see @fig-zi-plot).

```{r}
#| label: tbl-marg-zib-brms
#| tbl-cap: ""

marg_zi_brms <- fit_zi |>
  avg_comparisons(
    variables = "Fluency",
    dpar = "zi",
    comparison = "difference"
  )
```

```{r}
#| label: fig-zi-plot
#| fig-cap: "Visualization of the predicted difference for zero-inflated part of model"
#| echo: false

marg_zi_brms |>
  posterior_draws() |>
  ggplot(aes(x = draw, y = contrast)) +
  stat_halfeye(.width = c(.5, .8, .95), alpha = 0.7) +
  scale_fill_okabe_ito() +
  scale_x_continuous(labels = label_percent()) +
  labs(
    x = "Accuracy",
    y = "Fluency",
    caption = "50%, 80% and 95% credible intervals shown in black"
  ) +
  geom_vline(xintercept = 0, color = "black", linetype = "solid", size = 1.2) +
  theme_publication() +
  theme(legend.position = "none")
```

## Zero-One-Inflated Beta (ZOIB)

The ZIB model works well if you have 0s in your data, but not 1s.[^3] In our previous example, we nudged our 1s to .99. @kubinec2022 showed that this practice can result in serious distortion of the outcome as the sample size 𝑁 grows larger, resulting in ever smaller values that are “nudged”. Because the Beta distribution is a non-linear model of the outcome, values that are very close to the boundary, such as 0.00001 or 0.99999, will be highly influential outliers. Sometimes it is theoretically useful to model both zeros and ones as separate processes or to consider these values as essentially similar parts of the continuous response, as we show later in the ordered Beta regression model. For example, this is important in visual analog scale data where there might be a prevalence of responses at the bounds [@kong2016], in JOL tasks [@wilford2020], or in a free-list task where individuals provide open responses to some question or topic which are then recoded to fall between 0-1 [@bendixen2023]. Here 0s and 1s are meaningful; 0 means item was not listed and 1 means the item was listed first.

[^3]: In cases where your data include exact 1s but no 0s, you can fit a one-inflated beta regression model in `brms` by setting the `coi` parameter to `1`. This tells the model that all point masses occur at 1, rather than being split between 0 and 1. In other words, `coi = 1` assumes that any inflation in the data is due entirely to values at 1. In our data, we have exactly one value equal to 1\[\^6\]. While probably not significant to alter our findings, we can model ones with a special type of model called the zero-one-inflated beta (ZOIB) model [@liu2015] if we believe that both 0s and 1s are distinct outcomes.

Similar to our beta and zero-inflated models above, we can fit a ZOIB model in brms quite easily using the `zero_one_inflated_beta` family. In this model, we simultaneously estimate the mean ($\mu$) and precision ($\phi$) of the Beta distribution, a zero-one inflation parameter ($\alpha$) that represents the probability that an observation is either exactly 0 or 1 (i.e., 0 or 1 vs. not 0 or 1) and a conditional one-inflation parameter ($\gamma$) that represents the probability that, given an observation is at one of the endpoints, it is 1 (i.e, 1 vs. not 1). This specification captures the entire range of possible values while remaining constrained between 0 and 1. To get a better sense of how $\alpha$ and $\gamma$ control the distribution of values, @fig-zoib presents simulated data across combinations of these parameters. As $\alpha$ increases, we see a greater proportion of responses at the endpoints. As $\gamma$ increases, the proportion of endpoint responses at 1 grows relative to 0, making the spikes at 1 more prominent as $\gamma$ approaches 1. This visualization illustrates how the ZOIB model flexibly accounts for both the continuous portion of the distribution and the occurrence of exact 0s and 1s.

```{r}
#| label: fig-zoib
#| fig-cap: Simulated data from a ZOIB model illustrating the effects of the zero-one inflation parameter ($\alpha$) and the conditional one-inflation parameter ($\gamma$).
#| fig-width: 8
#| fig-height: 8

# Zero-one inflated beta generator adapted from
# https://vuorre.com/posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/
rzoib <- function(n = 1000, alpha = 0.1, gamma = 0.45, mu = 0.7, phi = 3) {
  a <- mu * phi
  b <- (1 - mu) * phi
  y <- ifelse(
    rbinom(n, 1, alpha),
    rbinom(n, 1, gamma),
    rbeta(n, a, b)
  )
  y
}

# Optimized parameter grid
param_grid <- expand_grid(
  alpha = c(0.1, 0.4),
  gamma = c(0.1, 0.9),
  mu = 0.7,
  phi = 3
)

# Simulate data
set.seed(123)
sim_data <- param_grid %>%
  mutate(
    data = pmap(
      list(alpha, gamma, mu, phi),
      ~ rzoib(1000, ..1, ..2, ..3, ..4)
    )
  ) %>%
  unnest(data) %>%
  rename(Accuracy = data)

# Classify type
plot_data <- sim_data %>%
  mutate(
    Type = case_when(
      Accuracy == 0 ~ "Zero",
      Accuracy == 1 ~ "One",
      TRUE ~ "Beta"
    )
  )

# Ensure 'Zero' comes before 'One'
plot_data <- plot_data %>%
  mutate(Type = factor(Type, levels = c("Zero", "One", "Beta")))

ggplot() +
  # Beta histogram
  geom_histogram(
    data = plot_data %>% filter(Type == "Beta"),
    aes(x = Accuracy, y = after_stat(count)),
    bins = 50,
    fill = "gray60",
    color = "white",
    alpha = 0.8
  ) +
  # Spikes at 0 and 1
  geom_col(
    data = plot_data %>%
      filter(Type != "Beta") %>%
      group_by(alpha, gamma, Accuracy, Type) %>%
      summarise(count = n(), .groups = "drop"),
    aes(x = Accuracy, y = count, fill = Type),
    width = 0.01
  ) +
  scale_fill_manual(
    values = c("Zero" = "red", "One" = "blue"),
    name = "Type"
  ) +
  facet_grid(
    gamma ~ alpha,
    labeller = label_both,
    scales = "free_y",
    switch = "y"
  ) +
  labs(
    x = "Accuracy",
    y = "Count"
  ) +
  theme_publication() +
  scale_x_continuous(labels = label_percent())
```

To fit a ZOIB model we use the `bf()` function. We model each parameter as a function of Fluency. We then pass the `zoib_model` to our `brm()` function (see @lst-zoib). The summary of the output is in @tbl-all (under `ZOIB`).

```{r}
#| label: fit zoib model
#| lst-label: lst-zoib
#| lst-cap: Fitting a ZOIB model with `brm()`.
#| echo: true
#| results: hide
#| message: false

# fit the zoib model

zoib_model <- bf(
  Accuracy ~ Fluency, # The mean of the 0-1 values, or mu
  phi ~ Fluency, # The precision of the 0-1 values, or phi
  zoi ~ Fluency, # The zero-or-one-inflated part, or alpha
  coi ~ Fluency, # The one-inflated part, conditional on the 0s, or gamma
  family = zero_one_inflated_beta()
)

fit_zoib <- brm(
  formula = zoib_model,
  data = fluency_data,
  file = "bayes_zoib_model"
)
```

```{r}
#| label: tbl zoib
#| echo: false
#| after-caption-space: 0pt
#| eval: false

zoib_model <- model_parameters(fit_zoib, "mean")

zoib_model |>
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))),
    `95% CI` = str_glue("[{CI_low}, {CI_high}]")
  ) |>
  select(Parameter, Mean, `95% CI`, pd) |>
  tt(digits = 3) |>
  format_tt(escape = TRUE)
```

### Model Parameters

The output for the model is pretty lengthy we are estimating four parameters each with their own independent responses and sub-models. All the coefficients are on the logit scale, except $\phi$ , which is on the log scale. Thankfully drawing inferences for all these different parameters, plotting their distributions, and estimating their average marginal effects looks exactly the same--all the `brms` and `marginaleffects` functions we used work the same.

### Predictions and Marginal Effects

With `marginaleffects` we can choose marginalize over all the sub-models, averaged across the 0s, continuous responses, and 1s in the data, or we can model the parameters separately using the `dpar` argument like we did above setting it to: $\mu$, $\phi$, $\alpha$, $\gamma$ (see below). Using `avg_predictions()` and not setting `dpar` we can get the predicted probabilities across all the sub-models. We can also plot the overall difference between fluency and disfluency for the whole model with `plot_predictions()`.

We wont highlight all the parameters for this model $\mu$, $\phi$ and $\alpha$ are the same as above, but below I show how one can extract the predicted probabilities and marginal effects for $\gamma$

```{r}
#| label: avg coi
#| lst-label: lst-coi
#| lst-cap: "Extacting predicted probablities and marginal effects for conditional-one parameter"
#| echo: true

# get average predictions for coi param
coi_probs <- avg_predictions(fit_zoib, by = c("Fluency"), dpar = "coi")
# get differene between the two conditions
coi_me <- avg_comparisons(fit_zoib, variables = c("Fluency"), dpar = "coi")
```

## Ordered Beta Regression

Looking at the output from the ZOIB model (@tbl-all), we can see how running a model like this can become vastly complex and computational intensive as it is fitting sub-models for each parameter. The ability to consider 0s and 1s as distinct processes from continuous values comes at a price in terms of complexity. A special version of the ZOIB was recently developed called ordered beta regression [@kubinec2022]. The ordered beta regression model allows for the analysis of continuous data (between 0-1) and discrete outcomes (e.g., 0 or 1) without requiring that either be fully distinct from the other. In the simplest sense, the ordered beta regression model is a hybrid model that estimates a weighted combination of a beta regression model for continuous responses and a logit model for the discrete values of the response.

The weights that average together the two parts of the outcome (i.e., discrete and continuous) are determined by cutpoints that are estimated in conjunction with the data in a similar manner to what is known as an ordered logit model. An in-depth explanation of ordinal regression is beyond the scope of this tutorial [but see @Fullerton2023; @bürkner2019]. At a basic level, ordinal regression models are useful for outcome variables that are categorical in nature and have some inherent ordering (e.g., Likert scale items). To preserve this ordering, ordinal models rely on the cumulative probability distribution. Within an ordinal regression model it is assumed that there is a continuous but unobserved latent variable that determines which of $k$ ordinal responses will be selected. For example on a typical Likert scale from 'Strongly Disagree' to 'Strongly Agree', you could assume that there is a continuous, unobserved variable called 'Agreement'.

While we cannot measure Agreement directly, the ordinal response gives us some indication about where participants are on the continuous Agreement scale. $k - 1$ cutoffs are then estimated to indicate the point on the continuous Agreement scale at which your Agreement level is high enough to push you into the next ordinal category (say Agree to Strongly Agree). Coefficients in the model estimate how much different predictors change the estimated *continuous* scale (here, Agreement). Since there’s only one underlying process, there’s only one set of coefficients to work with (proportional odds assumption). In an ordered beta regression, three ordered categories are modeled: (1) exactly zero, (2) somewhere between zero and one, and (3) exactly one. In an ordered beta regression, (1) and (2) are modeled with cumulative logits, where one cutpoint is the the boundary between Exactly 0 and Between 0 and 1 and the other cutpoint is the boundary between *Between 0 and 1* and *Exactly 1.* Somewhere between 0-1 (3) is modeled as a beta regression with parameters reflecting the mean response on the logit scale. Ultimately, employing cutpoints allows for a smooth transition between the bounds and the continuous values, permitting both to be considered together rather than modeled separately as the ZOIB requires.

The ordered beta regression model has shown to be more efficient and less biased than some of the methods discussed [@kubinec2022] herein and has seen increasing use across the biomedical and social sciences [@wilkesQuantifyingCoextinctionsEcosystem2024; @martinVocalComplexitySocially2024; @smithEcologicalMomentaryAssessment2024; @shresthaBigFiveTraits2024; @nouvianGlyphosateImpairsAversive2023] because it produces only a single set of coefficient estimates in a similar manner to a standard beta regression or OLS.

### Fitting an Ordered Beta Regression

To fit an ordered Beta regression in a Bayesian context we use the `ordbetareg` [@ordbetareg] package. `ordbetareg` is a front-end to the `brms` package that we described earlier; in addition to the functions available in the package, most `brms` functions and plots, including the diverse array of regression modeling options, will work with `ordbetareg` models. We first load the `ordbetareg` package (see @lst-ordbeta).

```{r}
#| echo: true
#| lst-label: lst-ordbeta
#| lst-cap: Load `ordbetareg`
#| label: load ordbetareg
#| eval: false

library(ordbetareg)
```

The `ordbetareg` package uses `brms` on the front-end so all the arguments we used previously apply here. Instead of the `brm()` function we use `ordbetareg()`. To fit a model where dispersion does not vary as a function of fluency we can use the below code (see @lst-fitordbeta).

```{r}
#| label: brms-ordered-beta-model
#| lst-label: lst-fitordbeta
#| lst-cap: Fitting ordered beta model with `ordbetareg()`
#| results: hide
#| echo: true

ord_fit_brms <- ordbetareg(
  Accuracy ~ Fluency,
  data = fluency_data,
  file = "bayes_ordbeta_model"
)
```

However, if we want dispersion to vary as a function of fluency we can easily do that (see @lst-ordbetadisp). Note the addition of the `phi_reg` argument in `m.phi`. This argument allows us to include a model that explicitly models the dispersion parameter. Because we are modeling $\phi$ as a function of fluency, we set the the argument to `both.`

```{r}
#| label: brms ordered beta model phi
#| lst-label: lst-ordbetadisp
#| lst-cap: Fitting ordered beta model with dispersion using `ordbetareg()`
#| results: hide
#| echo: true

ord_beta_phi <- bf(Accuracy ~ Fluency, phi ~ Fluency)

m.phi <- ordbetareg(
  ord_beta_phi,
  data = fluency_data,
  phi_reg = 'both',
  file = "bayes_ordbeta_phi_model"
)
```

## Marginal effects

@tbl-all presents the overall model summary (under `Ordered Beta`). We can use `marginaleffects` to calculate differences on the response scale that average over (or marginalize over) all our parameters.

```{r}
#| label: fit ordbeta

ord_beta_avg <- m.phi |>
  avg_comparisons(
    variables = "Fluency",
    comparison = "difference"
  )
```

```{r}
#| label: tbl-ordbeta
#| tbl-cap: "Marginal effect for fluency factor in ordered Beta model"

ord_beta_avg |>
  select(term, contrast, estimate, conf.low, conf.high) |>
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))),
    `95% CI` = str_glue("[{conf.low}, {conf.high}]")
  ) |>
  rename("Contrast" = "contrast", "Mean" = "estimate", "Term" = "term") |>
  select(Term, Contrast, Mean, `95% CI`) |>
  tt() |>
  format_tt(digits = 3) |>
  format_tt(escape = TRUE)

```

In @tbl-ordbeta the credible interval is close enough to zero relative to its uncertainty that we can conclude there likely aren’t substantial differences between the conditions after taking dispersion and the zeros and ones in our data into account.

### Cutpoints

The model cutpoints are not reported by default in the summary output, but we can access them with the R package `posterior` [@posterior2025a] and the functions `as_draws` and `summary_draws`.

```{r}
#| label: tbl-cutpoints
#| tbl-cap: Cutzero and cutone parameter summary
#| after-caption-space: 0pt
#| echo: false

as_draws(m.phi, c("cutzero", "cutone")) |>
  summarize_draws() |>
  select(variable, mean, q5, q95) |>
  mutate(across(c(mean, q5, q95), ~ round(.x, 2))) |>
  mutate(`95% CI` = str_glue("[{q5}, {q95}]")) |>
  rename("Parameter" = "variable", "Mean" = "mean") |>
  select(Parameter, Mean, `95% CI`) |>
  tt() |>
  format_tt(escape = TRUE)

```

In @tbl-cutpoints, `cutzero` is the first cutpoint (the difference between 0 and continuous values) and `cutone` is the second cutpoint (the difference between the continuous values and 1). These cutpoints are on the logit scale and as such the numbers do not have a simple substantive meaning. In general, as the cutpoints increase in absolute value (away from zero), then the discrete/boundary observations are more distinct from the continuous values. This will happen if there is a clear gap or bunching in the outcome around the bounds. This type of empirical feature of the distribution may be useful to scholars if they want to study differences in how people perceive the ends of the scale versus the middle.

### Model Fit

The best way to visualize model fit is to plot the full predictive distribution relative to the original outcome. Because ordered beta regression is a mixed discrete/continuous model, a separate plotting function, `pp_check_ordbetareg`, is included in the `ordbetareg` package that accurately handles the unique features of this distribution. The default plot in `brms` will collapse these two features of the outcome together, which will make the fit look worse than it actually is. The `ordbetareg` function returns a list with two plots, `discrete` and `continuous`, which can either be printed and plotted or further modified as `ggplot2` objects (see @fig-ppcheckord).

```{r}
#| label: fig-ppcheckord
#| fig-width: 12
#| fig-height: 8
#| fig-cap: Posterior predictive check for ordered beta regression model. A. Discrete posterior check. B. Continuous posterior check.

plots <- pp_check_ordbeta(
  m.phi,
  ndraws = 100,
  outcome_label = "Final Test Accuracy"
)

plot_grid(plots$discrete, plots$continuous, labels = c("A", "B"))
```

The discrete plot which is a bar graph, shows that the posterior distribution accurately captures the number of different types of responses (discrete or continuous) in the data. For the continuous plot shown as a density plot with one line per posterior draw, the model does a very good job at capturing the distribution.

Overall, it is clear from the posterior distribution plot that the ordered beta model fits the data well. To fully understand model fit, both of these plots need to be inspected as they are conceptually distinct.

#### Model Visualization

`ordbetareg` provides a visualization function called `plot_hess`. This function produces a plot of predicted proportions across the range of our `Fluency` factor. In @fig-heiss we get predicted proportions for Fluency across the bounded scale. Looking at the figure we can see there is much overlap between instructors in the middle portion ($\mu$) . However, we do see some small differences at the zero bounds.

```{r}
#| label: fig-heiss
#| fig-cap: "Heiss Plot of Predicted Probablities across the scale (0-100)"

plot_heiss(
  ord_fit_brms,
  grouping_fac = "Fluency",
  recode_group_labels = c("Fluency", "Disfluency")
)
```

### Ordered Beta Scale

In the `ordbetareg` function there is a `true_bound` argument. In the case where you data in not bounded between 0-1, you can use the argument to specify the bounds of the argument to fit the ordered beta regression. For example, you data might be bounded between 1 and 7. If so, you can model it as such.

```{r}
#| eval: false

# this code runs all models from above and creates table. This does not run each time and it set not to run.

# BAYESIAN REGRESSION
brms_model_new <- model_parameters(bayes_ols_model, centrality = "mean") %>%
  filter(Parameter != "sigma") %>%
  mutate(
    across(where(is.numeric), ~ round(.x, 3)),
    `95% Cr.I` = str_glue("[{Cr.I_low}, {CI_high}]"),
    Model = "Bayesian LM"
  ) %>%
  select(Model, Parameter, Mean, `95% CI`, pd)

# BETA REGRESSION
beta_model_new <- model_parameters(beta_brms, centrality = "mean") %>%
  mutate(
    across(where(is.numeric), ~ round(.x, 3)),
    `95% CI` = str_glue("[{CI_low}, {CI_high}]"),
    Model = "Beta Regression"
  ) %>%
  select(Model, Parameter, Mean, `95% CI`, pd)

# BETA REGRESSION
beta_model_dis <- model_parameters(beta_brms_dis, centrality = "mean") %>%
  mutate(
    across(where(is.numeric), ~ round(.x, 3)),
    `95% CI` = str_glue("[{CI_low}, {CI_high}]"),
    Model = "Beta Regression"
  ) %>%
  select(Model, Parameter, Mean, `95% CI`, pd)

# ZERO-INFLATED REGRESSION
zi_model_new <- model_parameters(fit_zi, centrality = "mean") %>%
  mutate(
    across(where(is.numeric), ~ round(.x, 3)),
    `95% CI` = str_glue("[{CI_low}, {CI_high}]"),
    Model = "ZIB"
  ) %>%
  select(Model, Parameter, Mean, `95% CI`, pd)

# ZERO-ONE INFLATED BETA REGRESSION
zoib_model_new <- model_parameters(fit_zoib, centrality = "mean") %>%
  mutate(
    across(where(is.numeric), ~ round(.x, 3)),
    `95% CI` = str_glue("[{CI_low}, {CI_high}]"),
    Model = "ZOIB"
  ) %>%
  select(Model, Parameter, Mean, `95% CI`, pd)


# BETA REGRESSION
ord_beta <- model_parameters(m.phi, centrality = "mean") %>%
  mutate(
    across(where(is.numeric), ~ round(.x, 3)),
    `95% CI` = str_glue("[{CI_low}, {CI_high}]"),
    Model = "Ordered Beta"
  ) %>%
  select(Model, Parameter, Mean, `95% CI`, pd)
# ZERO-INFLATED REGRESSION

# COMBINE ALL MODELS
combined_table <- bind_rows(
  brms_model_new,
  beta_model_dis,
  zi_model_new,
  zoib_model_new,
  ord_beta
)

# Make value column with estimate + CI + pd
combined_table_clean <- combined_table %>%
  mutate(
    value = str_glue("{Mean} {`95% CI`} (pd = {pd})")
  ) |>
  select(Parameter, Model, value) %>%
  pivot_wider(names_from = Model, values_from = value) |>
  mutate(across(everything(), ~ replace_na(.x, "-")))

# View
combined_table_clean |>
  tt(digits = 2) |>
  format_tt(escape = TRUE) |>
  save_tt(
    here("manuscript", "Figures", "combined_table.png"),
    overwrite = TRUE
  )
```

# Discussion

The use of Beta regression in psychology, and the social sciences in general, is rare. With this tutorial, we hope to turn the tides. Beta regression models are an attractive alternative to models that impose unrealistic assumptions like normality, linearity, homoscedasticity, and unbounded data. Beyond these models, there are a diverse array of different models that can be used depending on your outcome of interest.

Throughout this tutorial our main aim was to help guide researchers in running analyses with proportional or percentage outcomes using Beta regression and some of it's alternatives. In the current example, we used real data from @wilford2020 and discussed how to fit these models in R, interpret model parameters, extract predicted probabilities and marginal effects, and visualize the results.

Comparing our analysis with that of @wilford2020, we demonstrated that using traditional approaches (e.g., *t*-tests) to analyze accuracy data can lead to inaccurate inferences. Although we successfully reproduced one of their key findings, our use of Beta regression and its extensions revealed important nuances in the results. With a traditional Beta regression model--which accounts for both the mean and the precision (dispersion)--we observed similar effects of instructor fluency on performance. However, the standard Beta model does not accommodate boundary values (i.e., 0s and 1s).

When we applied a ZIB model, which explicitly accounts for structural zeros, we found no effect of fluency on the mean ($\mu$) part of the model. Instead, the effect of fluency emerged in the structural zero (inflated zero; $\alpha$) component. This pattern was consistent when using a zero-one-inflated Beta (ZOIB) model. Furthermore, we fit an ordered Beta regression model [@kubinec2022], which appropriately models the full range of values, including 0s and 1s. Here, we did not observe a reliable effect of fluency on the mean once we accounted for dispersion.

These analyses emphasize the importance of fitting a model that aligns with the nature of the data. The simplest and recommended approach when dealing with data that contains zeros and/or ones is to fit an ordered Beta model, assuming the process is truly continuous. However, if you believe the process is distinct in nature, a ZIB or ZOIB model might be a better choice. Ultimately, this decision should be guided by theory.

For instance, if we believe fluency influences the structural zero part of the model, we might want to model this process separately using a ZIB or ZOIB. With the current dataset, fluency might affect specific aspects of performance (such as the likelihood of complete failure) rather than general performance levels. This effect could be due to participant disengagement during the disfluent lecture. If students fail to pay attention because of features of disfluency, they may miss relevant information, leading to a floor effect at the test. If this is the case, we would want to model this appropriately. However, if we believe fluency effects general performance levels, a model that takes in to account the entire process accounting for the zeros and ones might be appropriate.

In the discussion section of @wilford2020, they were unable to offer a tenable explanation for performance differences based on instructor fluency. A model that accounts for the excess zeros in the dataset provides one testable explanation: watching a disfluent lecture may lead to lapses in attention, resulting in poorer performance in that group. These lapses, in turn, contribute to the observed differences in the fluent condition. This modeling approach opens a promising avenue for future research--one that would have remained inaccessible otherwise.

Not everyone will be eager to implement the techniques discussed herein. In such cases, the key question becomes: What is the least problematic approach to handling proportional data? One reasonable option is to fit multiple models tailored to the specific characteristics of your data. For example, if your data contain zeros, you might fit two models: a traditional OLS regression excluding the zeros, and a logistic model to account for the zero versus non-zero distinction. If your data contain both zeros and ones, you could fit separate models for the zeros and ones in addition to the OLS model. There are many defensible strategies to choose from depending on the context. However, we do not recommend transforming the values of your data (e.g., 0s to .01 and 1s to .99) or ignoring the properties of your data simply to fit traditional statistical models.

In this tutorial, we demonstrated how to analyze these models from a Bayesian perspective. While we recognize that not everyone identifies as a Bayesian, implementing these models using a Bayesian framework is relatively straightforward--it requires only a single package, lowering the barrier to entry. For those who prefer frequentist analyses, several R packages are available. For standard beta regression, the `betareg` package [@betareg] is a solid option, while more complex models such as zero-inflated and ordered beta regressions can be implemented using `glmmTMB` [@glmmTMB]. For fitting zero-one models, there is a new implementation in @betareg, that allows you to model these types of data.

## Conclusion

Overall, this tutorial emphasizes the importance of modeling the data you have. Although the example provided is relatively simple (a one-factor model with two levels), we hope it demonstrates that even with a basic dataset, there is much nuance in interpretation and inference. Properly modeling your data can lead to deeper insights, far beyond what traditional measures might offer. With the tools introduced in this tutorial, researchers now have the means to analyze their data effectively, uncover patterns, make accurate predictions, and support their findings with robust statistical evidence. By applying these modeling techniques, researchers can improve the validity and reliability of their studies, ultimately leading to more informed decisions and advancements in their respective fields.

# References

::: {#refs}
:::
