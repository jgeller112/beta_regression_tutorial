---
title: "A Beta Way: A Tutorial For Using Beta Regression in Psychological Research"
shorttitle: "Beta Regression Tutorial"
author:
  - name: Jason Geller
    corresponding: true
    orcid: 0000-0002-7459-4505
    email: drjasongeller@gmail.com
    affiliations:
      - name: Boston College
        department: Department of Psychology and Neuroscience
        address: McGuinn 300
        city: Chestnut Hill
        region: MA
        country: USA
        postal-code: 02467
    roles:
      - conceptualization
      - writing
      - data curation
      - editing
      - formal analysis
  - name: Robert Kubinec
    orcid: 0000-0001-6655-4119
    affiliations:
      - name: University of South Carolina
    roles:
      - writing
      - editing
      - formal analysis
  - name: Chelsea M. Parlett Pelleriti
    orcid: 0000-0001-9301-1398
    affiliations:
      - name: School
    roles:
      - writing
      - editing
      - formal analysis
  - name: Matti Vuorre
    orcid: 0000-0001-5052-066X
    affiliations:
      - name: Tilburg University
    roles:
      - writing
      - editing
      - formal analysis
author-note:
  status-changes: 
    affiliation-change: ~
    deceased: ~
  disclosures:
    # Example: This study was registered at X (Identifier Y).
    study-registration: "No preregistration for this paper."
    # Acknowledge and cite data/materials to be shared.
    data-sharing: Data, code, and materials for this manusscript can be found at. 
    related-report: ~
    conflict-of-interest: The authors have no conflicts of interest to disclose.
    financial-support: ~
    authorship-agreements: ~
abstract: "Rates, percentages, and proportions are common outcomes in psychology and the social sciences. These outcomes are often analyzed using models that assume normality, but this practice overlooks important features of the data, such as their natural bounds at 0 and 1. As a result, estimates can become distorted. In contrast, treating such outcomes as beta-distributed respects these limits and can yield more accurate estimates. Despite these advantages, the use of beta models in applied research remains limited. Our goal is to provide researchers with practical guidance for adopting beta regression models, illustrated with an example drawn from the psychological literature. We begin by introducing the beta distribution and beta regression, emphasizing key components and assumptions. Next, using data from a learning and memory study, we demonstrate how to fit a Beta regression model in R with the Bayesian package `brms` and how to interpret results using the `marginaleffects` package. We also discuss model extensions, including zero-inflated, zero- and one-inflated, and ordered beta models. To promote wider adoption of these methods, we provide detailed code and materials at https://github.com/jgeller112/beta_regression_tutorial."
keywords: [beta regression, beta distribution,R,tutorial, psychology, learning and memory]
floatsintext: true
# Numbered lines (.pdf and .docx only)
numbered-lines: false
# File with references
suppress-title-page: false
# Link citations to references
link-citations: true
# Masks references that appear in the masked-citations list
mask: false
word-count: true 
# Language options. See https://quarto.org/docs/authoring/language.html
lang: en
language:
  citation-last-author-separator: "and"
  citation-masked-author: "Masked Citation"
  citation-masked-date: "n.d."
  citation-masked-title: "Masked Title"
  email: "Email"
  title-block-author-note: "Author Note"
  title-block-correspondence-note: "Correspondence concerning this article should be addressed to"
  title-block-role-introduction: "Author roles were classified using the Contributor Role Taxonomy (CRediT; https://credit.niso.org/) as follows:"
bibliography: bibliography.bib
format:
  apaquarto-pdf: 
    documentmode: doc
  apaquarto-docx: default
  apaquarto-html: default
execute: 
  echo: false
  warning: false
  message: false
  results: hide
  freeze: auto
  fig-align: "center"
  tbl-align: "center"
  code-line-numbers: true
  keep-with-next: true
  code-overflow: wrap
  fig-width: 8
  fig-height: 4 
knitr:
  opts_chunk: 
    dev: "ragg_png"
---

In psychological research, it is common to run experiments or administer surveys in which the primary outcomes of interest—such as task performance, attitudes, or choices—are expressed as proportions or percentages. For instance, in educational and cognitive research, one popular way to assess learning is by looking at the proportion of correct responses on a test. To illustrate, consider a memory experiment where participants read a short passage on a specific topic. After a brief distractor task, they complete a final memory test consisting of 10 short-answer questions, each assigned a different point value (e.g., question 1 might be worth 4 points, while question 2 might be worth 1 point). The primary outcome measure could be the proportion of points earned on each question relative to the total possible points for each question.

A key question arises: how should proportional data like this be analyzed? In psychology, such outcomes are often analyzed using models from the general linear model (GLM) or generalized linear model (GLiM) frameworks. General linear models — including t-tests, ANOVAs, and linear regression — assume the outcome is normally distributed, unbounded, and exhibits constant variance. However, these assumptions are frequently violated when working with proportional data, which are bounded between 0 and 1 and tend to show non-constant variance, especially near the boundaries [@ferrari2004; @paolino2001]. This can lead to biased estimates and invalid inferences, making general linear models an unsuitable choice for analyzing proportions.

Another option are GLiMs, which extend the linear model framework to accommodate non-normal outcome distributions and different link functions. For example, binomial or Bernoulli models with a logit link (commonly referred to as logistic regression) are well-suited for binary outcomes or counts of successes out of a fixed number of trials. However, these models may still fall short when proportions are treated as continuous outcomes or when data exhibit overdispersion or cluster near 0 or 1.

The challenges of analyzing proportional data are not new [see @bartlett1936]. Fortunately, several alternative approaches address the limitations of commonly used models. One such approach is Beta regression, an extension of the GLiM that employs the Beta distribution (described in-depth below) [@ferrari2004; @paolino2001]. Beta regression offers a flexible and robust solution for modeling proportional data by accounting for boundary effects and overdispersion, making it a valuable alternative to traditional binomial models. This approach is particularly well-suited for psychological research because it can handle both the bounded nature of proportional data and the non-constant variance often encountered in these datasets.

## A Beta Way Forward

With the combination of open-source programming languages like R \[\@R\] and a vibrant community of package developers, analyses such as Beta regression have become increasingly accessible. Yet, adoption of these methods—particularly in psychology—remains limited. One reason may be the lack of resources tailored to the needs of psychologists conducting applied research. Although recent years have seen a surge of interest in Beta regression [@heiss2021; @vuorre2019; @bendixen2023; @coretta2025bayesian], most introductions focus narrowly on a small subset of Beta regression models. This tutorial aims to address that gap by offering a more comprehensive overview of Beta regression and its extensions, along with practical tools and code to help psychologists apply these methods in their own work. While previous tutorials have largely focused on interpreting model summaries, we emphasize the importance of interpreting estimates on the response scale.

In this tutorial, we provide (a) give a brief, non-technical overview of the principles underlying beta regression, (b) walk-through an empirical example of applying Beta regression in the popular R programming language and (c) highlight the the extensions which are most relevant to researchers in psychology (e.g., zero-inflated, zero-one-inflated, and ordered Beta regressions). Moreover, we provide a fully reproducible code supplement at <https://github.com/jgeller112/beta_regression_tutorial> that provides more detailed code examples.[^1]

[^1]: In this article, we try to limit code where possible; however, the online version has all the code needed to reproduce all analyses herein. Furthermore, to promote transparency and reproduciblity, the tutorial was written in R version 4.4.3 (@R) using Quarto (v.1.5.54), an open-source publishing system that allows for dynamic and static documents. This allows figures, tables, and text to be programmatically included directly in the manusCr.Ipt, ensuring that all results are seamlessly integrated into the document. In addition, we use the `rix` [@rix] R package which harnesses the power of the `nix` [@nix] ecosystem to to help with computational reproducibility. Not only does this give us a snapshot of the packages used to create the current manusCr.Ipt, but it also takes a snapshot of system dependencies used at run-time. This way reproducers can easily re-use the exact same environment by installing the `nix` package manager and using the included default.nix file to set up the right environment. The README file in the GitHub repository contains detailed information on how to set this up to reproduce the contents of the current manusCr.Ipt, including a video.

## Bayesian Approach to Beta Regression

Beta regression models can be estimated from either a frequentist or Bayesian perspective. In this tutorial, we adopt a Bayesian framework—not because it is inherently superior, but because it offers practical advantages for fitting both simple and complex models [@gelman2013bayesian; @mcelreath2020statistical; @bayes_rules], especially in the context of psychological data. The R package `brms` [@brms], which interfaces with the probabilistic programming language Stan [@stan2023], makes Bayesian modeling accessible and flexible, even for users with limited prior experience in Bayesian statistics.

At the heart of Bayesian estimation is the idea of updating beliefs. This invovles first specifying prior distributions that represent our beliefs about the parameters before observing any data. These priors are then updated through the likelihood function using the observed data, producing posterior distributions. The posterior reflects what we believe about the parameters after taking the data into account. Unlike frequentist approaches that yield point estimates and confidence intervals, Bayesian methods return full distributions over parameters. These distributions allow researchers to express uncertainty more directly, make probability-based claims, and explore the full range of plausible values for each estimate.

The models we fit throughout this tutorial are estimated using `brms`, which supports a wide range of regression families, including the Beta distribution and its extensions. It also allows for flexible model specification, including hierarchical structures, non-default link functions, and separate sub-models for different components. Although the underlying estimation relies on Markov Chain Monte Carlo (MCMC) sampling, `brms` handles this process seamlessly.

Once a model is fit, the results are typically summarized using a variety of different metrics, such as point estimates from the posterior distribution (i.e., mean/median/MAP), credible intervals (Cr.Is), probablity of direction (pd), and region of practical equivales (ROPE), and Bayes factors, to name a few [@makowski2019].

Throughout the tutorial, we will demonstrate how to fit these Bayesian Beta regression models using the `brm()` function. We will also focus on interpreting the results in terms of probabilities and proportions on the original response scale, rather than staying solely on the logit or latent scale.

## Beta Distribution

Proportional data are common in psychology, but they pose challenges for standard modeling approaches. That is, the data are bounded between 0 and 1 and often exhibit non-constant variance (heteroscedasticity) [@ferrari2004; @paolino2001]. Common distributions used within the GLM or GLiM frameworks often fail to capture these properties adequately, which can necessitate alternative modeling strategies.

In any statistical model, the expected value (or mean) of the response variable serves as the central estimand we aim to describe and predict. The model specifies how this expected value depends on explanatory variables through several components: a random component that specifies the distribution of the response variable around its expected value (such as a Poisson or binomial, which are part of the exponential family), a linear predictor that combines explanatory variables in a linear form, and a link function that connects the mean of the response variable to the linear predictor (also referred to as the expected value) [@nelder1972]. Together, these components provide a flexible framework for modeling data with different distributional properties.

The Beta distribution offers a compelling solution [@paolino2001]. The Beta distribution is continuous, restricted to values between 0 and 1 (exclusive), and highly flexible. Its two shape parameters—commonly called shape1 ($\alpha$) and shape2 ($\beta$)—govern the distribution’s location, skewness, and spread. By adjusting its two shape parameters it can take on many functional forms (e.g., it can be symmetric, skewed, U-shaped, or even approximately uniform; see @fig-curves).

To see this intuitively, consider a test question worth six points. Suppose a participant scores four out of six. The number of points received (4) can be treated as $\alpha$, and the number of points missed (2) as $\beta$. The resulting Beta distribution would be skewed toward higher values, reflecting a high performance. Reversing these values would produce a distribution skewed toward lower values, representing poorer performance (see @fig-beta-dist). These parameterizations offer a natural way to visualize performance data as probability densities.

```{r}
#| label: packages_1
#| echo: false
#| message: false
#| warning: false

# packages needed
library(tidyverse) # tidy functions/data wrangling/viz
library(easystats)
library(scales) # percentage
library(tinytable) # tables
library(marginaleffects) # marginal effects
library(extraDistr) # Use extra distributions like dprop()
library(brms) # bayesian models
library(posterior) # cutpoints
library(bayesplot) # bayes
library(ggdist) # stat_halfeye viz bayes
library(patchwork)# combine figure
library(cowplot) # combine figures
library(gghalves) # box plot
library(ggbeeswarm) # raw data plot

options(scipen = 999) # get rid of scientific notation
```

```{r}
#| label: ggplot theme
#| echo: false
# Define the custom publication theme

theme_publication <- function(base_size = 14, base_family = "Times") {
    theme(
      # General text
      text = element_text(size = base_size, family = base_family, color = "black"),
      
      # Titles
      plot.title = element_text(face = "bold", hjust = 0.5, size = base_size + 2),
      plot.subtitle = element_text(hjust = 0.5, size = base_size),
      plot.caption = element_text(hjust = 0.5, size = base_size - 2),
      
      # Axis
      axis.title = element_text(face = "bold", size = base_size),
      axis.title.x = element_text(margin = margin(t = 10)), # Add space below axis title
      axis.title.y = element_text(margin = margin(r = 10)), # Add space beside axis title
      axis.text = element_text(size = base_size - 1),
      axis.line = element_line(color = "black"),
      axis.ticks = element_line(color = "black"),
      # Panel
      panel.grid.major = element_line(color = "gray80"),
      panel.grid.minor = element_blank(), # APA style minimizes gridlines
      panel.background = element_blank(),
      
      # Legend
      legend.position = "bottom",
      legend.title = element_text(face = "bold", size = base_size),
      legend.text = element_text(size = base_size - 1),
      legend.background = element_blank(),
      
      # Strip (for facets)
      strip.text = element_text(face = "bold", size = base_size),
      strip.background = element_rect(fill = "gray90", color = "black")
    )
}

```

```{r}
#| label: fig-curves
#| fig-cap: "Examples of beta distributions with different shape parameters"
# Create base plot over the interval [0,1]
p <- ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
  stat_function(fun = dbeta, args = list(shape1 = 0.5, shape2 = 0.5), 
                aes(color = "shape1=0.5, shape2=0.5"), size = 1.2) +
  stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 2), 
                aes(color = "shape1=2, shape2=2"), size = 1.2) +
  stat_function(fun = dbeta, args = list(shape1 = 5, shape2 = 1), 
                aes(color = "shape1=5, shape2=1"), size = 1.2) +
  stat_function(fun = dbeta, args = list(shape1 = 1, shape2 = 3), 
                aes(color = "shape1=1, shape2=3"), size = 1.2) +
  stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 5), 
                aes(color = "shape1=2, shape2=5"), size = 1.2) +
  labs(
    x= "Proportion",
    y = "Density", 
    color = "Parameters"
  ) +
  theme_publication()


p

ggsave("fig-shapes.png", path=here::here("manuscript", "Figures"), dpi=300)

```

```{r}
#| label: fig-beta-dist
#| fig-cap: "A. beta distribution with 4 correct (`shape1`) and 2 incorrect (`shape2`) responses (`shape2`) on one test question. B. beta distribution with 2 correct (`shape1`) and 4 incorrect (`shape2`) responses on one test question"
#| echo: false
#| message: false

# Create the plot with a filled beta distribution density
p1 <- ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
  stat_function(
    fun = dbeta, args = list(shape1 = 4, shape2 = 2),
    geom = "area", fill = "blue", alpha = 0.5
  ) +
  stat_function(
    fun = dbeta, args = list(shape1 = 4, shape2 = 2),
    aes(color = "beta(shape1 = 4, shape2 = 2)")
  ) +
  theme_publication() +
  theme(legend.position = "none") +
  labs(x = "Proportion Correct on Question", y = "Density")

# Create the plot with a filled beta distribution density
p2 <- ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
  stat_function(
    fun = dbeta, args = list(shape1 = 2, shape2 = 4),
    geom = "area", fill = "red", alpha = 0.5
  ) +
  stat_function(
    fun = dbeta, args = list(shape1 = 2, shape2 = 4),
    aes(color = "beta(shape1 = 4, shape2 = 2)")
  ) +
  theme_publication() +
  theme(legend.position = "none") +
  labs(x = "Proportion Correct on Question", y = "Density") 

plot_grid(p1, p2, labels = c("A", "B"))

ggsave("fig-test-beta.png", path=here::here("manuscript", "Figures"), dpi=300)
```

### Mean and Precision

While $\alpha$ and $\beta$ are useful for defining the distribution, regression modeling typically uses a reparameterized form involving the **mean** $\mu$ and **precision** $\phi$ . The mean ($\mu$) represents the expected value of the distribution, while the dispersion ($\phi$), which is inversely related to variance, reflects how concentrated the distribution is around the mean, with higher values of $\phi$ representing a narrower distribution while lower values of $\phi$ represent a wider, more spread out, distribution. These are connected to $\alpha$ and $\beta$ as follows:

$$
\begin{aligned}[t]
\text{Shape 1:} && a &= \mu \phi \\
\text{Shape 2:} && b &= (1 - \mu) \phi
\end{aligned}
\qquad\qquad\qquad
\begin{aligned}[t]
\text{Mean:} && \mu &= \frac{a}{a + b} \\
\text{Precision:} && \phi &= a + b
\end{aligned}
$$

The variance can then be calculated as a function of $\mu$ and $\phi$:

$$
\frac{\mu \cdot (1 - \mu)}{1 + \phi}
$$

Importantly, *the variance depends on the average value of the response*, which is what allows the model to non-linearly adjust to the bounds of the outcome.

### Beta Regression

We can use the Beta distribution in regression models to describe a continuous response variable that is bounded between 0 and 1. Although the Beta distribution applies to values in this interval, it is straightforward to rescale other variables to fit this range using normalization formulas.

In Beta regression, the goal is to model the mean of the response variable (often denoted $\mu$) as a function of predictor variables, while optionally modeling a precision or dispersion parameter (often denoted $\phi$) that captures variability around the mean. To ensure that $\mu$ stays between 0 and 1, we apply a link function, which transforms the mean so that it can be modeled on an unbounded scale. A common choice is the logit link (which is used by default), but other links (such as the probit or complementary log-log) are also available; the logit is not required.

The logit link is defined as $\text{logit}(\mu) = \log \left( \frac{\mu}{1 - \mu} \right)$. This transforms the mean proportion, $\mu$, into log-odds, which can take any real value between $-\infty$ and $+\infty$. This makes it suitable for linear modeling. The inverse of the logit, called the logistic function, maps predictions back onto the 0 to 1 scale: $\left[ \mu = \frac{1}{1 + e^{-\eta}} \right]$ where $\eta$ is the linear predictor (a combination of your model’s coefficients and predictors). Similarly, the dispersion parameter, which must be positive, is often modeled using a log link: $\log(\phi) = \text{linear predictor}$. This ensures the estimated dispersion is strictly positive.

Beta regression is designed specifically for modeling bounded, continuous outcomes. It respects the (0, 1) range of proportional data, accounts for varying variance across levels of the mean, and provides a coherent statistical foundation for estimating effects on proportions and percentages. This makes it especially useful in psychology, where outcomes like accuracy, recall rates, or rating scales often fall within this bounded range.

# Motivating Example

## Data and Methods

To demonstrate Beta models, we re-analyze data from a real memory experiment by @wilford2020, which tested how the fluency of an instructor influenced final recall performance. This example allows us to walk through each modeling step, from basic regression to advanced Beta-based alternatives.” The principles of Beta regression are best understood in the context of a real data set. The example we are gonna use comes from the learning and memory literature. A whole host of literature has shown extrinsic cues like fluency (i.e., how easy something is to process) can influence metamemory (i.e., how well we think we will remember something). As an interesting example, a line of research has focused on instructor fluency and how that influences both metamemory and actual learning. When an instructor uses lots of non-verbal gestures, has variable voice dynamics/intonation, is mobile about the space, and includes appropriate pauses when delivering content, participants perceive them as more fluent, but it does not influence actual memory performance, or what we learn from them [@carpenter2013; @toftness2017; @witherby2022]. While fluency of instructor has not been found to impact actual memory across several studies, @wilford2020 found that it can. In several experiments, @wilford2020 showed that when participants watched two videos of a fluent vs. a disfluent instructor, they remembered more information on a final test. Given the interesting, and contradictory results, we chose this paper to highlight. In the current tutorial we are going to re-analyze the final recall data from Wilford et al. (2020; Experiment 1A; data: <https://osf.io/6tyn4/>).

In @wilford2020 (Experiment 1A), they presented participants with two short videos highlighting two different concepts: (1) genetics of calico cats and (2) an explanation as to why skin wrinkles. Participants viewed either disfluent or fluent versions of these videos.[^2] For each video, metamemory was assessed using judgements of learning (JOLs). JOLs require participants to rate an item on scale between 0-100 with 0 representing the item will not be remembered and a 100 representing they will definitely remember the item. In addition, other questions about the instructor were assessed and how much they learned. After a distractor task, a final free recall test was given were participants had to recall as much information about the video as they could in 3 minutes. Participants could score up to 10 points for each video. They looked at the proportion of information recalled (out of 10) as their outcome measure.

[^2]: See an example of the fluent video here: <https://osf.io/hwzuk>. See an example of the disfluent video here: <https://osf.io/ra7be>.

Proprotional data is widely used in psychology and is well suited for Beta regression. Despite this, it is common to treat proportional data as continuous and unbounded, and analyze the resulting proportions using methods that fall under the general linear model. Below we will reproduce the analysis conducted by @wilford2020 (Experiment 1A) and then re-analyze it using Beta regression. We hope to show how Beta regression and its extensions can be a more powerful tool in making inferences about your data.

In @wilford2020 (Experiment 1A), they presented participants with two short videos highlighting two different concepts: (1) genetics of calico cats and (2) an explanation as to why skin wrinkles. Participants viewed either disfluent or fluent versions of these videos.[^3] For each video, metamemory was assessed using JOLs. JOLs require participants to rate an item on scale between 0-100 with 0 representing the item will not be remembered and a 100 representing they will definitely remember the item. In addition, other questions about the instructor were assessed and how much they learned. After a distractor task, a final free recall test was given were participants had to recall as much information about the video as they could in 3 minutes. Participants could score up to 10 points for each video. They looked at the proportion of information recalled (out of 10) as their outcome measure. Here we will be looking at the proportion recalled on the final test.

[^3]: See an example of the fluent video here: <https://osf.io/hwzuk>. See an example of the disfluent video here: <https://osf.io/ra7be>.

## Reanalysis of Wilford et al. Experiment 1A

In Experiment 1A, @wilford2020 only used the first time point (one video) and compared fluent and disfluent conditions with a *t*-test. They found better performance for participants watching the fluency instructor than the disfluency instructor (see @fig-flu1). In our re-analysis, we will run a Bayesain regression model that is comparable to analysis used by @wilford2020. Specifically, we will examine accuracy on final test as our DV (because the score was on a 10 point scale we divided by 10 to get a proportion) and look at fluency of the instructor as our outcome measure (Fluent vs. Disfluent). We used the default treatment (dummy) coding in R, which sets the first level of a factor (alphabetically) as the reference level. In this case, the disfluent level served as the reference. In addition we used default (noninformative priors).

```{r}
#| label: data
#| echo: false

fluency_data <- read.csv("data/miko_data.csv") |>
  # Rename the columns for better readability
  rename(
    "Participant" = "ResponseID", # Rename "ResponseID" to "Participant"
    "Fluency" = "Condition", # Rename "Condition" to "Fluency"
    "Time" = "name", # Rename "name" to "Time"
    "Accuracy" = "value" # Rename "value" to "Accuracy"
  ) |>
  # Transform the data
  mutate(
    Accuracy = Accuracy / 10, # Convert Accuracy values to proportions
    Fluency = ifelse(Fluency == 1, "Fluent", "Disfluent"),
  ) |>
  filter(Time == "FreeCt1AVG") |> # Only choose first time point
  select(-X, -Time) |> # Drop unnecessary columns
  mutate(Participant = dense_rank(Participant)) |> # Assign sequential participant numbers
  relocate(Accuracy, .after = last_col()) # Move Accuracy to the end

```

```{r}
#| label: tbl-dataset
#| tbl-cap: "First six rows of dataset"
#| echo: false
#| after-caption-space: 0pt
# Display the first few rows of the modified dataframe
# underscore won't display in Latex
head(fluency_data) |>
  tt() |>
  format_tt(escape=TRUE)
```

```{r}
#| label: fig-flu1
#| fig-cap: "Fluency accuracy: Raw data (points), half-boxplot, and Mean marker (diamond)"
#| echo: false
#| 
ggplot(fluency_data, aes(x = Fluency, y = Accuracy, fill = Fluency)) +
   geom_half_point(aes(color = Fluency), 
                  transformation = ggbeeswarm::position_quasirandom(width = 0.1),
                  side = "l", size = 1, alpha = 0.5) +
  geom_half_boxplot(
    side = "r"
  ) +
  stat_summary(
    fun = mean,
    geom = "point",
    shape = 23,  # diamond shape
    size = 4,
    color = "blue",
    fill = "blue"
  ) +
  scale_y_continuous(labels = scales::label_percent()) +
  ggokabeito::scale_fill_okabe_ito() +
  theme_publication() +
  theme(legend.position = "none")

ggsave("fig-accuracy-data.png", path=here::here("manuscript", "Figures"), dpi=300)
```

### Data

```{r}
#| label: setup
#| message: false
#| warning: false
packages <- c("tidyverse", "easystats", "scales", "tinytable", "marginaleffects","cowplot", "ggbeeswarm", "gghalves","webshot2",  "extraDistr", "brms", "posterior", "bayesplot", "ggdist")
```

```{r}
purrr::walk(packages, library, character.only = TRUE)
```

```{r}
options(scipen = 999)
```

Before we fit the regression model, we will read in the data.

```{r}

```

The first few rows of the data are displayed in @tbl-dataset. The dataset contains three columns:

-   **`Participant:`** Participant ID number

-   **`Fluency`:** Fluent vs. Disfluent

-   **`Accuracy`:** Proportion recalled (idea units)

```{r}
#| label: models
#| eval: false
# fit each model to put in table
# fit ols reg

bayes_ols_model <- brm(Accuracy ~ Fluency,
                       data = fluency_data,
                       cores=4,
                       backend = "cmdstanr",
                       file = "model_ols_bayes")

beta_brms_dis <- brm(model_beta_bayes_disp,
  data = data_beta,
  family = Beta(),
  chains = 4,
  iter = 2000,
  warmup = 1000,
  cores = 4,
  seed = 1234,
  backend = "cmdstanr",
  file = "model_beta_bayes_dis_run" # Save this so it doesn't have to always rerun
)

fit_zoib <- brm(
  formula = zoib_model,
  data = fluency_data,
  chains = 4, iter = 2000, warmup = 1000,
  cores = 4, seed = 1234,
  backend = "cmdstanr",
  file = "model_beta_zoib_1"
)


fit_zoib <- brm(
  formula = zoib_model,
  data = fluency_data,
  chains = 4, iter = 2000, warmup = 1000,
  cores = 4, seed = 1234,
  backend = "cmdstanr",
  file = "model_beta_zoib_1"
)


m.phi <- ordbetareg(bf(Accuracy ~ Fluency, 
                       phi ~ Fluency),
                    data=fluency_data,
                    backend = "cmdstanr",
                     file = "model_beta_ordbeta_phi", 
                    iter = 2000, 
                    cores=4, 
                    phi_reg='both') # log phi
```

```{r}
#| label: combined models
#| eval: false

# BAYESIAN REGRESSION
brms_model_new <- model_parameters(bayes_ols_model, centrality = "mean") %>%
  filter(Parameter != "sigma") %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)),
         `95% Cr.I` = str_glue("[{CI_low}, {CI_high}]"),
         Model = "Bayesian LM") %>%
  select(Model, Parameter, Mean, `95% Cr.I`, pd)

# BETA REGRESSION
beta_model_new <- model_parameters(beta_brms, centrality = "mean") %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)),
         `95% Cr.I` = str_glue("[{CI_low}, {CI_high}]"),
         Model = "Beta Regression") %>%
  select(Model, Parameter, Mean, `95% Cr.I`, pd)

# BETA REGRESSION
beta_model_dis <- model_parameters(beta_brms_dis, centrality = "mean") %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)),
         `95% Cr.I` = str_glue("[{CI_low}, {CI_high}]"),
         Model = "Beta Regression") %>%
  select(Model, Parameter, Mean, `95% Cr.I`, pd)

# ZERO-INFLATED REGRESSION
zi_model_new <- model_parameters(fit_zi, centrality = "mean") %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)),
         `95% Cr.I` = str_glue("[{CI_low}, {CI_high}]"),
         Model = "ZIB") %>%
  select(Model, Parameter, Mean, `95% Cr.I`, pd)

# ZERO-ONE INFLATED BETA REGRESSION
zoib_model_new <- model_parameters(fit_zoib, centrality = "mean") %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)),
         `95% Cr.I` = str_glue("[{CI_low}, {CI_high}]"),
         Model = "ZOIB") %>%
  select(Model, Parameter, Mean, `95% Cr.I`, pd)


# BETA REGRESSION
ord_beta <- model_parameters(m.phi, centrality = "mean") %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)),
         `95% Cr.I` = str_glue("[{CI_low}, {CI_high}]"),
         Model = "Ordered Beta") %>%
  select(Model, Parameter, Mean, `95% Cr.I`, pd)
# ZERO-INFLATED REGRESSION


# COMBINE ALL MODELS
combined_table <- bind_rows(
  brms_model_new,
  beta_model_dis, 
  zi_model_new,
  zoib_model_new, 
  ord_beta
)

# Make value column with estimate + CI + pd
combined_table_clean <- combined_table %>%
    mutate(
        value = str_glue("{Mean} {`95% Cr.I`} (pd = {pd})")
    ) |>
    select(Parameter, Model, value) %>%
    pivot_wider(names_from = Model, values_from = value) |>
    mutate(across(everything(), ~ replace_na(.x, "-")))

# View
combined_table_clean |> 
  tt(digits = 3) |>
  format_tt(escape = TRUE) |> 
    save_tt(here::here("manuscript", "Figures", "combined_table.png"))
```

```{r}
#| label: tbl-all
#| tbl-cap: "Regression model summaries for each model"
#| tbl-align: "center"
#| apa-note: "Mean[95% Cr.I](pd)"

knitr::include_graphics(here::here("manuscript","Figures",   "combined_table.png"))

```

## Bayesian Regression

To fit our Bayesian regression models, we will be using the `brms` [@brms] R package. For reference, lists each model that was run, along with the model parameters and posterior summaries, including the mean, 95% credible intervals (Cr.I), and posterior probabilities of direction (pd). For the following analyses we will be using default priors provided by `brms`, which are non-informative or weak. This will get us something tantamount to a frequentist model with maximum likelihood estimates most of the readers should be familiar with. For the purposes of this tutorial, we will not be getting into the minutiae of Bayesian data analysis (i.e., setting informative priors, MCMC sampling, etc,). For a more in-depth look into Bayesian data analysis I refer the reader to @mcelreath2020statistical and @bayes_rules.

To create a regression model from in `brms` we will use the `brm()` function and fit a model looking at final test accuracy (`Accuracy`) as a function of instructor fluency (`fluency`). The syntax is similar to how you run a regression using the popular `lm` function in R. Here we are concerned with modeling mean performance differences between the fluency and disfluency conditions.

We first start by loading the `brms` [@brms] and `cmdstanr` [@cmdstanr] packages. We use the `cmdstanr` backend for Stan [@stan2023] because it's faster and more modern than the defaults used to run models.[^4]

[^4]: In order to use the `cmdstanr` backend you will need to first install the package and also run `cmdstanr::install_cmdstan()` if you have not done so already.

```{r}
#| echo: true
# load brms and cmdstanr
library(brms)
library(cmdstanr) 
```

```{r}
#| eval: false
cmdstanr::install_cmdstan()
```

```{r}
#| echo: false
options(marginaleffects_posterior_center = mean)
# get mean instead of median in marginaleffects
```

We fit the model using the `brm()` function, as shown in the code below. To speed up processing we set `cores` = 4, and use the `cmdstand` backend. The output provides estimates for each effect: the coefficient (`b_,`) representing the mean of the posterior distribution), its estimation error (the standard deviation of the posterior distribution), and the 95% credible interval (Cr.I), which indicate the most credible values within the distribution. We inferred evidence for an effect when the 95% Cr.I did not include zero.

```{r}
#| label: Bayesian_regression_model
#| echo: true
#| warning: false
#| message: false
#| cache: true
#| results: hide

# fit ols reg
bayes_ols_model <- brm(Accuracy ~ Fluency,
                       data = fluency_data,
                       cores=4,
                       backend = "cmdstanr",
                       file = "model_ols_bayes")
```

```{r}
#| label: tbl brms ols
#| tbl-cap-location: top
#| echo: false
#| after-caption-space: 0pt

# for regression model

brms_model_new <- model_parameters(bayes_ols_model, centrality="mean")

```

@tbl-all displays the summary of the Bayesian regression model. To make the output more readable, each model parameter is labeled with a prefix before the variable name. The `b_Intercept` value refers to the accuracy in the disfluent condition (because we dummy coded our variable). The Fluency coefficient (`b_FluencyFluent`) highlights the difference between the fluent and disfluent conditions, *b* = `r round(brms_model_new$Mean[2], 3)`, 95% Cr.Is = \[`r round(brms_model_new$CI_low[2], 3)`,`r round(brms_model_new$CI_high[2], 3)`\], pd =`r round(brms_model_new$pd[2])`. Our results are very similar to the results reported by @wilford2020 : *t*(94) = 2.00, *p = .048,* 95% CI \[0.06, 16.56\].

There are some notable differences between our Bayesian analysis and the frequenist analyses readers might be more sued to seeing, such as the absence of *t*- and *p*-values. There is a metric in the table that is included with models fit when using the `bayestestr` [@makowski2019] function from `easystats` called probability of direction (pd) that gives an indication of how much of the posterior distribution estimate is in one direction (positive or negative). The pd measure appears to correlated with *p*-values (see [@makowski2019; @makowski2019a]). A pd of 95%, 97.5%, 99.5% and 99.95% correspond approximately to two-sided p-value of respectively .1, .05, .01 and .001. In addition to the pd value, one can look at the 95% credible interval (sometimes called highest probability density, depending on the package being used) to see if it includes 0--if it does not then the effect can be said to be significant. In the table below, the 95% credible intervals are located in the 95% Cr.Is column.

## Beta Regression

@wilford2020, using a traditional GLM approach, observed that instructor fluency impacts actual learning. Keep in mind the traditional approach assumes normality of residuals and homoscadacity or constant variance. These assumptions are tricky to maintain when the continuous response approaches either the upper or lower boundary of the scale and are almost never true (see [@sladekova2024unicorns]. Does the data we have meet those assumptions? Using `easystats` [@easystats] and the `check_model()` function, we can easily check this. In @fig-ols-assump , we see there definitely some issues with our model. Specifically, there appears to be violations of normality constant variance (homogeneity).

We can also examine how well the data fits the model by performing a posterior predictive check. In @fig-ppcheck-normal, multiple draws or repetitions from the posterior distribution are plotted (light blue lines) against the observed data. (dark green). Ideally, the predictive draws (the light blue lines) should show reasonable resemblance with the observed data (dark blue line). In our example, the model-predicted density is slightly too peaked and narrow compared to the data. In addition, some of the draws extend into negative accuracy values.

```{r}
#| label: fig-ols-assump
#| fig-cap: "Two assumption checks for our OLS model: Normality (left) and Homogeneity (right)"
#| echo: false

check_model(bayes_ols_model, check = c( "homogeneity", "normality"))
```

```{r}
#| label: fig-ppcheck-normal
#| fig-cap: "Posterior predictive check"
#| echo: false

check_model(bayes_ols_model, check = "pp_check")

```

Given the outcome variable is proportional, one solution would be to run a beta regression model.

Again, we can create the beta regression model in `brms`. In `brms`, we model each parameter independently. Recall from the introduction that in a Beta model we model two parameters—$\mu$ and $\phi$. We can easily do this by using the `bf()` function from `brms`. `bf()` facilitates the specification of several sub-models within the same formula call. We fit two formulas, one for $\mu$ and one for $\phi$ and store it in the `model_beta_bayes` object below. In the below `bf()` call, we are modeling Fluency as a function of Accuracy only for the $\mu$ parameter. For the $\phi$ parameter, we are only modeling the intercept value. This is saying dispersion does not change as a function of fluency.

```{r}
#| label: brms model beta
#| echo: true
# fit model with mu and phi
model_beta_bayes <- bf(
  Accuracy ~ Fluency, # fit mu model
  phi ~ 1 # fit phi model
) 
```

If we try to run `beta_brms`, however, we get an error: `Error: Family 'beta' requires response greater than 0`. This is by design. If your remember, the Beta distribution can only model responses in the interval \[0-1\], but not responses that are exactly 0 or 1. We need make sure there are no zeros and ones in our dataset.

```{r}
#| label: brms beta model
#| results: hide
#| eval: false

# run model

beta_brms <- brm(model_beta_bayes,
  data = fluency_data,
  family = Beta(),
  chains = 4,
  cores = 4,
  seed = 1234,
  backend = "cmdstanr",
  file = "model_beta_bayes_reg" # Save this so it doesn't have to always rerun
)
```

```{r}
#| label: tbl-01s
#| tbl-cap: Number of zeros and ones in our dataset
#| echo: false
#| after-caption-space: 0pt


fluency_data |>
  filter(Accuracy == 0 | Accuracy == 1) |> # get onlys 0s and 1s
  count(Accuracy) |> # count them
  rename("Count" = "n") |> 
  tt() |>
   format_tt(escape = TRUE)
```

@tbl-01s shows we have 9 rows with accuracy of 0, and 1 row with an accuracy of exactly 1. To run a Beta regression, a common hack is to nudge our 0s towards .01 and our 1s to .99 so they fall within the interval of \[0-1\]. The model `beta_brms` uses data where 0s and 1s are changed to .01 and .99, respectively.

```{r}

# transform 0 to 0.1 and 1 to .99
data_beta <- fluency_data |>
  mutate(
    Accuracy = ifelse(Accuracy == 0, .01, 
                      ifelse(Accuracy == 1, .99, Accuracy))
  )
```

We can rerun our model replacing `fluency_data` with `data_beta` in the below function call.

```{r}
#| label: beta_brms_01s
#| results: hide
#| cache: true
beta_brms <- brm(model_beta_bayes,
  data = data_beta,
  family = Beta(),
  chains = 4,
  cores = 4,
  backend = "cmdstanr",
  file = "model_beta_bayes_reg_1" # Save this so it doesn't have to always rerun
)
```

No errors this time! We will perform the Beta regression using the nudged values of .01 and .99 values and report our results.

#### Model Parameters

@tbl-all displays the summary from our Bayesian implementation. [^5] The $\mu$ parameter estimates, which are labeled with an underscore `b_`while $\phi$ parameter coefficients are tagged with b`_phi` in the `Parameter` column.

[^5]: We have chain diagnostics included like Rhat and ESS which indicates how the MCMC sampling performed. For more information check out Gelman et al., 2013; Kruschke, 2014; McElreath, 2020)

```{r}
#| label: beta-brms
#| echo: false
#| eval: false
#| after-caption-space: 0pt

model_parameters(beta_brms, "mean") |>
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))), 
  `95% Cr.I` = str_glue("[{CI_low}, {CI_high}]")) |> 
  select(Parameter, Mean, `95% Cr.I`, pd) |> 
  tt() |>
  format_tt(digits = 3) |>
  format_tt(escape = TRUE)
```

### Mean $\mu$

```{r}
#| label: beta-mu
#| eval: false
#| echo: false
#| after-caption-space: 0pt


model_parameters(beta_brms, "mean") |>
  filter(!startsWith(Parameter, "b_phi")) |> 
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))), 
  `95% Cr.I` = str_glue("[{CI_low}, {CI_high}]")) |> 
  select(Parameter, Mean, `95% Cr.I`, pd) |> 
  tt() |>
  format_tt(digits = 3) |>
  format_tt(escape = TRUE)

```

In @tbl-all under the Beta Regression column, the first set of coefficients represent how factors influence the $\mu$ parameter, which is the mean of the beta distribution. These coefficients are interpreted on the scale of the logit, meaning they represent linear changes on a nonlinear space. The intercept term `(b_Intercept)` represents the log odds of the mean on accuracy for the fluent instructor. Log odds that are negative indicate that it is more likely a "success" (like getting the correct answer) will NOT happen than that it will happen. Similarly, regression coefficients in log odds forms that are negative indicate that an increase in that predictor leads to a decrease in the predicted probability of a "success".

#### Predicted Probabilities

Parameter estimates are usually difficult to intercept on their own. We argue that researchers should not spend too much time interpreting single model estimates. We report them in this tutorial for completeness. Instead researchers should discuss the effects of the predictor on the actual outcome of interest (in this case the 0-1 scale). The logit link allows us to transform back and forth between the scale of a linear model and the nonlinear scale of the outcome, which is bounded by 0 and 1. By using the inverse of the logit, we can easily transform our linear coefficients to obtain average effects on the scale of the proportions or percentages, which is usually what is interesting to applied researchers. In a simple case, we can do this manually, but when there are many factors in your model this can be quite complex.

To help us extract predictions from our model and visualize them we will use a package called **`marginaleffects`** [@margeffects2024]. [^6] To get the proportions for each of our categorical predictors on the mu parameter we can use the function from the package called `predictions()`. These are displayed in @tbl-predict-prob.

[^6]: `ggeffects` is another great package to extract marginal effects and plot [@ggeffects-2]

```{r}
#| echo: true
# load marginaleffects package
library(marginaleffects)
```

```{r}
#| eval: false
#| echo: true

# use predictions to get predicted probs for each condition
predictions(beta_brms,
  newdata = datagrid(Fluency = c("Disfluent", "Fluent")) # need to specify the levels of the categorical predictor
)
```

```{r}
#| label: tbl-predict-prob
#| tbl-cap: Predicted probablities for fluency factor
#| tbl-cap-location: top
#| echo: false
#| after-caption-space: 0pt

# get the predicted probablities for each level of fluency
predictions(beta_brms,
  newdata = datagrid(Fluency = c("Disfluent", "Fluent"))
) |>
  mutate(Fluency = ifelse(Fluency == "Fluent", "Fluent", "Disfluent")) |>
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))), 
         `95% Cr.I` = str_glue("[{conf.low}, {conf.high}]")) |>
  rename("Mean" = "estimate") |> 
  select(Fluency, Mean, `95% Cr.I`) |> 
  relocate(Fluency) |>
  tt(digits = 3)|>
  format_tt(escape=TRUE)
```

For the `Fluency` factor, we can interpret the `Mean` column in terms of proportions or percentages. That is, participants who watched the fluent instructor scored on average 36% on the final exam compared to 26% for those who watched the disfluent instructor.

We can also easily visualize these from `marginaleffects` using the `plot_predictions` function. After using this function, the proportions are visualized in @fig-plot-pre.

```{r}
#| label: plot-beta
#| echo: true
beta_plot <- plot_predictions(beta_brms, condition = "Fluency")
```

```{r}
#| label: fig-plot-pre
#| fig-cap: Predicted probablities for fluency factor

beta_plot + 
  scale_x_discrete(
    breaks = c("0", "1"),
    labels = c("Fluent", "Disfluent")
  ) +
  labs(x="Fluency") + 
  scale_y_continuous(labels = label_percent()) + 
  theme_publication()

ggsave("fig-plot-pre.png", path=here::here("manuscript", "Figures"), dpi=300,)
```

Readers might notice that @fig-plot-pre displays a single point estimate along with its 95% Cr.I.. In Bayesian data analysis, MCMC sampling generates a distribution for each parameter of interest. This approach allows us to visualize the uncertainty surrounding our estimates. Using the `marginaleffects` package, we can obtain samples from the posterior distribution with the `posterior_draws()` function. We can then plot these results to illustrate the range of plausible values for our estimates at different levels of uncertainty (e.g., 80% or 95%) (see @fig-draws-fluency).

```{r}
# Add a model identifier to each dataset
pred_draws_beta <- avg_predictions(beta_brms, variables = "Fluency") |>  
  posterior_draws()
```

```{r}
#| label: fig-draws-fluency
#| fig-cap: Predicted probablity posterior distributions by fluency 
# Plot both models
ggplot(pred_draws_beta, aes(x = draw, y = Fluency, fill= Fluency)) +
  stat_halfeye(.width = c(.5, .8, .95), alpha = 0.7) +
  ggokabeito::scale_fill_okabe_ito() +
  scale_x_continuous(labels = label_percent()) +
  labs(x = "Accuracy",
       y = "Fluency",
       caption = "50%, 80% and 95% credible intervals shown in black") +
  geom_vline(xintercept = 0, color = "black", linetype = "solid", size = 1.2) + 
  theme_publication() +
  theme(legend.position = "none")

ggsave("fig-draws-fluency.png", path=here::here("manuscript", "Figures"), dpi=300,)
```

#### Marginal Effects

Marginal effects provide a way to understand how changes in a predictor influence an outcome, holding all other factors constant in a specific manner. Technically, marginal effects are calculated using partial derivatives for continuous variables or finite differences for categorical and continuous variables, depending on the nature of the data and the research question. Substantively, these effects translate regression coefficients into a form that can be interpreted directly on the outcome scale of interest.

There are various types of marginal effects, and their calculation can vary across software packages. For example, the popular `emmeans` package \[\] computes marginal effects by holding all predictors at their means. In this tutorial, we will use the `marginaleffects` package, which focuses on average marginal effects (AMEs) by default. AMEs summarize effects by generating predictions for each row of the original dataset and then averaging these predictions. This approach retains a strong connection to the original data while offering a straightforward summary of the effect of interest.

One practical application of AMEs is calculating the risk difference for categorical variables. The risk difference represents the difference in average outcomes between two groups or conditions. Using the `avg_comparisons()` function in the `marginaleffects` package, we can compute this metric directly. By default, the function calculates the discrete difference between groups. It can also compute other effect size metrics, such as odds ratios and risk ratios, depending on the research question (see @tbl-or for more details). This flexibility makes it a powerful tool for interpreting regression results in a meaningful way.

```{r}
#| label: ame1
#| echo: true
# get risk difference by default
beta_avg_comp <- avg_comparisons(beta_brms)
```

```{r}
#| label: tbl-ame1
#| echo: false
#| tbl-cap: Risk difference for instructor fluency
#| after-caption-space: 0pt


beta_avg_comp |>
  # remove unwanted variables
  select(-predicted_lo, -predicted_hi, -tmp_idx, -predicted) |>
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))), 
         `95% Cr.I` = str_glue("[{conf.low}, {conf.high}]")) |>
  rename("Contrast" = "contrast","Mean" = "estimate", "Term" = "term") |>
  select(Term, Contrast, Mean,`95% Cr.I`) |> 
  tt(digits = 2) |>
  format_tt(escape=TRUE)
```

@tbl-ame1 displays the risk difference for the fluency factor (`estimate` column). The difference between the fluent and disfluent conditions is .09. That is, participants who watched a fluent instructor scored 9% higher on the final recall test than participants who watched the disfluent instructor. Our credible interval \[-.0174, -0.011\] does not include zero so we can say it is statistically significant.

We can also get the odds ratio with `avg_comparisons` (see @tbl-or).

```{r}
#| label: tbl-or
#| echo: false
#| tbl-cap: Odds ratio for fluency factor
#| after-caption-space: 0pt


# get odds ratios as an example
avg_comparisons(beta_brms,
  comparison = "lnoravg",
  transform = "exp"
) |>
  select(-predicted_lo, -predicted_hi, -tmp_idx, -predicted)|> 
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))), 
         `95% Cr.I` = str_glue("[{conf.low}, {conf.high}]")) |>
  rename("Contrast" = "contrast","Mean" = "estimate", "Term" = "term") |>
  select(Term, Contrast, Mean,`95% Cr.I`) |> 
  tt()  |>
   format_tt(escape = TRUE)
```

In psychology, it is common to report effect size measures like Cohen's *d* [@cohen1977]. When working with proportions we can calculate something similar called Cohen's *h*. Taking our proportions, we can use the below equation to calculate Cohen's *h* along with the 95% Cr.Is around it.

$$
h = 2 \cdot \left( \arcsin\left(\sqrt{p_1}\right) - \arcsin\left(\sqrt{p_2}\right) \right)
$$

```{r}
# Proportions
p1 <- 0.37  # Fluent
p2 <- 0.29  # Disfluent

# Cohen's h formula
cohens_h <- 2 * (asin(sqrt(p1)) - asin(sqrt(p2)))
round(cohens_h, 3)


#CI lower and upper bounds
p1_lower <- 0.328
p1_upper <- 0.419
p2_lower <- 0.250
p2_upper <- 0.329

# Lower bound of h
h_lower <- 2 * (asin(sqrt(p1_lower)) - asin(sqrt(p2_upper)))
# Upper bound of h
h_upper <- 2 * (asin(sqrt(p1_upper)) - asin(sqrt(p2_lower)))

```

Using this metric we see the effect size is small (`r round(cohens_h, 3)`), 95% Cr.I \[`r round(h_lower,3)`, `r round(h_upper,3)`\].

### Precision ($\phi$) Component

The other component we need to pay attention to is the dispersion or precision parameter coefficients labeled as \``b_phi` in @tbl-all. The dispersion ($\phi$) parameter tells us how precise our estimate is. Specifically, $\phi$ in Beta regression tells us about the variability of the response variable around its mean. Specifically, a higher dispersion parameter indicates a narrower distribution, reflecting less variability. Conversely, a lower dispersion parameter suggests a wider distribution, reflecting greater variability. The main difference between a dispersion parameter and the variance is that the dispersion has a different interpretation depending on the value of the outcome, as we show below. The best way to understand dispersion is to examine visual changes in the distribution as the dispersion increases or decreases.

Understanding the dispersion parameter helps us gauge the precision of our predictions and the consistency of the response variable. In `beta_brms` we only modeled the dispersion of the intercept. When $\phi$ is not specified, the intercept is modeled by default (see @tbl-phi-dis).

```{r}
#| label: tbl-phi-dis
#| tbl-cap: Dispersion parameter
#| after-caption-space: 0pt

model_parameters(beta_brms, "mean") |>
  filter(startsWith(Parameter, "b_phi")) |> 
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))), 
  `95% Cr.I` = str_glue("[{CI_low}, {CI_high}]")) |> 
  select(Parameter, Mean, `95% Cr.I`, pd)|>
  tt() |>
  format_tt(digits = 3) |>
   format_tt(escape = TRUE)
```

The intercept under the precision heading is not that interesting. It represents the overall dispersion in the model. We can model the dispersion of the `Fluency` factor—this allows dispersion to differ between the fluent and disfluent conditions. To do this we add `Fluency` to the phi model in `bf()`. We model the precision of the `Fluency` factor by using a `~` and adding factors of interest to the right of it.

```{r}
#| echo: true
model_beta_bayes_disp <- bf(
  Accuracy ~ Fluency, # fit mu model
  phi ~ Fluency # fit phi model with fluency 
) 
```

```{r}
#| label: brms-model-dis
#| results: hide
#| echo: true

beta_brms_dis <- brm(model_beta_bayes_disp,
  data = data_beta,
  family = Beta(),
  chains = 4,
  iter = 2000,
  warmup = 1000,
  cores = 4,
  seed = 1234,
  backend = "cmdstanr",
  file = "model_beta_bayes_dis_run" # Save this so it doesn't have to always rerun
)

```

```{r}
#| label: tbl beta mu dis
#| echo: false
#| after-caption-space: 0pt
#| eval: false 

model_parameters(beta_brms_dis, "mean") |>
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))), 
  `95% Cr.I` = str_glue("[{CI_low}, {CI_high}]")) |> 
  select(Parameter, Mean, `95% Cr.I`, pd) |> 
  tt() |>
  format_tt(digits = 3) |>
  format_tt(escape = TRUE)
```

@tbl-all displays the model summary with the precision parameter added to our model as a function of fluency. It is important to note that the estimates are logged and not on the original scale (this is only the case when additional parameters are modeled). To interpret them on the original scale, we can exponentiate the log-transformed value—this transformation gets us back to our original scale . In below model call, we set `exponentiate` = TRUE.

```{r}
#| label: phi-beta
#| echo: true
#| after-caption-space: 0pt

beta_model_dis_exp <- beta_brms_dis |>
  model_parameters(exponentiate=TRUE, centrality = "mean") 
```

```{r}
#| label: tbl-phi-beta-exp
#| echo: false
#| tbl-cap: Beta regression model summary for fluency factor with $\phi$ parameter exponentiated
#| after-caption-space: 0pt

beta_model_dis_exp |>
    filter(startsWith(Parameter, "b_phi")) |> 
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))), 
  `95% Cr.I` = str_glue("[{CI_low}, {CI_high}]")) |> 
  select(Parameter, Mean, `95% Cr.I`, pd) |> 
  tt(digits=3) |>
  format_tt(escape=TRUE)

```

The $\phi$ intercept represents the precision of the fluent condition. The $\phi$ coefficient for `Fluency1` represents the change in that precision for performance between the fluent vs. disfluent conditions. The Cr.I \[.235, .684\] do not include 0 so our results are statistically significant.

It is important to note that these estimates are not the same as the marginal effects we discussed earlier. Changes in dispersion will change the shape of the distribution but not necessarily the average value of the response. This makes dispersion most interesting for research questions that focus on other features of the distribution besides the mean, such as the level of polarization in an outcome.

A critical assumption of the GLM is homoscedasticity, which means constant variance of the errors. Here we see one of the benefits of a beta regression model: we can include a dispersion parameter for Fluency. Properly accounting for dispersion is crucial because it impacts the precision of our mean estimates and, consequently, the significance of our coefficients. The inclusion of dispersion in the our model increased the uncertainty of the $\mu$ coefficient (see @fig-plt-diff)--our Cr.I is now very close to zero. This suggests that failing to account for the dispersion of the variables might lead to biased estimates. This highlights the potential utility of an approach like Beta regression over a traditional approach as Beta regression can explicitly model dispersion and address issues of heteroscedasticity.

We won't always need to include dispersion parameters for each of our variables. We advise conducting very simple model comparisons like leave one out (loo) cross validation to examine if a dispersion parameter should be considered in our model.

```{r}
#| label: fig-plt-diff
#| fig-cap: "Comparison of posterior distributions for the risk difference in fluency: Simple model  (no dispersion for Fluency) vs. complex model with  dispersion"
#| echo: false
#| 

# Add a model identifier to each dataset
pred_no <- avg_comparisons(beta_brms, variables = "Fluency") |>  
  posterior_draws() |>
  mutate(model = "No_Disp")

pred_yes <- avg_comparisons(beta_brms_dis, variables = "Fluency") |> 
  posterior_draws() |>
  mutate(model = "Disp")

# Combine both datasets
pred_combined <- bind_rows(pred_no, pred_yes)

ggplot(pred_combined, aes(x = draw, y = model, fill = model)) +
  stat_halfeye(.width = c(.8, .95), alpha = 0.7) +
  ggokabeito::scale_fill_okabe_ito() +
  # guides(fill = guide_legend(title = "Model")) +  # Remove or comment out
  labs(x = "Fluency", 
       y = "Difference in Final Recall Performance",
       caption = "80% and 95% credible intervals shown in black") +
  geom_vline(xintercept = 0, color = "black", linetype = "solid", size = 1.2) + 
  theme_publication() +
  theme(legend.position = "none")

ggsave("fig-plot-dis.png", path=here::here("manuscript", "Figures"), dpi=300,)

```

### Posterior Predictive Check

It is a always a good idea to check how well your data fits the model. The `pp_check()` function allows us to examine the fit between our data and the model. In @fig-post-pred, multiple draws or repetitions from the posterior distribution are plotted (light blue lines) against the observed data. (dark blue). Ideally, the predictive draws (the light blue lines) should show reasonable resemblance with the observed data (dark blue line). We see it does a pretty good job capturing the data. Ideally, the predictive draws (the light blue lines) should show reasonable resemblance with the observed data (dark blue line). We see it does a pretty good job capturing the data.

```{r}
#| label: fig-post-pred
#| fig-cap: Posterior predictive check for our Beta model with 100 draws.

pp_check(beta_brms_dis, ndraws = 100)  + # ndraws is nu 
  theme_publication()
```

## Zero-Inflated Beta (ZIB) Regression

A limitation of the beta regression model is it can can only model values between 0 and 1, but not exactly 0 or 1. In our dataset we have 9 rows with `Accuracy` equal to zero.

To use the Beta distribution we nudged our zeros to 0.01--which is never a good idea in practice. In our case it might be important to model the structural zeros in our data, as fluency of instructor might be an important factor in predicting the zeros in our model. There is a model called the zero-inflated beta (ZIB) model that takes into account the structural 0s in our data. We’ll still model the $\mu$ and $\phi$ (or mean and precision) of the Beta distribution, but now we’ll also add one new special parameter: $\alpha$.

With zero-inflated regression, we’re actually modelling a mixture of the data-generating process. The $\alpha$ parameter uses a logistic regression to model whether the data is 0 or not. Substantively, this could be a useful model when we think that 0s come from a process that is relatively distinct from the data that is greater than 0. For example, if we had a dataset of with proportion of looks or eye fixations to certain areas on marketing materials, we might want a separate model for those that do not look at certain areas on the screen because individuals who do not look might be substantively different than those that look.

We can fit a ZIB model using `brms` and use the `marginaleffects` package to make inferences about our parameters of interest. Before we run a zero-inflated beta model, we will need to transform our data again and nudge our 1s to .99--we can keep our zeros. Similar to our Beta regression model we fit in `brms`, we will use the `bf()` function to fit several models. We fit our $\mu$ and $\phi$ parameters as well as our zero-inflated parameter ($\alpha$; here labeled as `zi`). In `brms` we can use the zero_inflated_beta family.

```{r}
#| label: change 1s
#| echo: false 
# keep 0 but transform 1 to .99
data_beta_0 <- fluency_data |>
  mutate(Accuracy = ifelse(Accuracy == 1, .99, Accuracy))
```

```{r}
#| label: zib brms model
#| echo: true
#|
# fit zero-inflated beta in brms
zib_model <- bf(
  Accuracy ~ Fluency, # The mean of the 0-1 values, or mu
  phi ~ Fluency, # The precision of the 0-1 values, or phi
  zi ~ Fluency, # The zero-or-one-inflated part, or alpha
  family = zero_inflated_beta()
)
```

Below we pass `zib_model` to the `brm` function.

```{r}
#| label: fit zib model
#| results: hide
#| echo: true
fit_zi <- brm(
  formula = zib_model,
  data = data_beta_0,
  cores = 4,
  iter = 2000,
  warmup = 1000,
  seed = 1234,
  backend = "cmdstanr",
  file = "model_beta_bayes_zib"
)
```

```{r}
#| label: tbl brms zib
#| echo: false
#| eval: false
#| after-caption-space: 0pt


model_parameters(fit_zi, "mean")  |>
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))), 
  `95% Cr.I` = str_glue("[{CI_low}, {CI_high}]")) |> 
  select(Parameter, Mean, `95% Cr.I`, pd) |> 
  tt() |>
  format_tt(digits = 3) |>
  format_tt(escape = TRUE)
```

### Predicted Probabilities and Marginal Effects

@tbl-all under the zero-inflated Beta regression column provides a summary of the posterior distribution for each parameter. As stated before, it is preferable to back-transform our estimates to get probabilities. To get the predicted probabilities we can use the `avg_predictions()` function and to get risk difference between the levels of each factor we can use the `avg_comparisons()` function from `marginaleffects` package [@marginaleffects]. We can model the parameters separately using the `dpar` argument setting to: $\mu$, $\phi$, $\alpha$. Here we look at the risk difference for Fluency under each parameter.

#### Mean ($\mu$)

Looking at @tbl-marg-mu-brms there is no significant effect for Fluency.

```{r}

marg_zi_brms <- fit_zi |>
  avg_comparisons(
    variables = "Fluency",
    dpar = "mu",
    comparison = "difference"
  )
```

```{r}
#| label: tbl-marg-mu-brms
#| tbl-cap:  Risk difference ($\mu$) for fluency factor
#| echo: false
#| after-caption-space: 0pt


marg_zi_brms |>
  select(term,contrast, estimate, conf.low, conf.high) |> 
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))), 
         `95% Cr.I` = str_glue("[{conf.low}, {conf.high}]")) |>
  rename("Contrast" = "contrast","Mean" = "estimate", "Term" = "term") |>
  select(Term, Contrast, Mean,`95% Cr.I`) |> 
   tt() |>
   format_tt(digits = 3) |>
   format_tt(escape = TRUE)

```

#### Dispersion ($\phi$)

Looking at @tbl-marg-phi-brms, there is a significant effect of fluency on dispersion, with disfluency having a larger variation than fluency.

```{r}

marg_phi_brms <- fit_zi |>
  avg_comparisons(
    variables = "Fluency",
    dpar = "phi",
    comparison = "difference"
  )
```

```{r}
#| label: tbl-marg-phi-brms
#| tbl-cap:  Risk difference ($\phi$) for fluency factor
#| echo: false
#| after-caption-space: 0pt


marg_phi_brms |>
  select(term,contrast, estimate, conf.low, conf.high) |> 
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))), 
         `95% Cr.I` = str_glue("[{conf.low}, {conf.high}]")) |>
  rename("Contrast" = "contrast","Mean" = "estimate", "Term" = "term") |>
  select(Term, Contrast, Mean,`95% Cr.I`) |>
   tt() |>
   format_tt(digits = 3) |>
   format_tt(escape = TRUE)

```

### Zero-Inflated ($\alpha$)

In @tbl-marg-zib-brms we see that watching a lecture video with a fluent instructor reduces the proportion of zeros by about 13%. The Cr.I does not include zero.

```{r}
#| label: marg-zib-brms 

marg_zi_brms <- fit_zi |>
  avg_comparisons(
    variables = "Fluency",
    dpar = "zi",
    comparison = "difference"
  )
```

```{r}
#| label: tbl-marg-zib-brms
#| tbl-cap: Regression coefficients for ZIB model (`marg_zi_brms`)
#| echo: false
#| after-caption-space: 0pt


marg_zi_brms |>
  select(term,contrast, estimate, conf.low, conf.high) |> 
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))), 
         `95% Cr.I` = str_glue("[{conf.low}, {conf.high}]")) |>
  rename("Contrast" = "contrast","Mean" = "estimate", "Term" = "term") |>
  select(Term, Contrast, Mean,`95% Cr.I`) |> 
   tt() |>
   format_tt(digits = 3) |>
   format_tt(escape = TRUE)
```

If we wanted we could easily visualize the zero-inflated part of the model (see @fig-zi-plot).

```{r}
#| label: fig-zi-plot
#| fig-cap: "Visualization of predicted probablites for zero-inflated part of model"
#| echo: false

zi_plot <- plot_predictions(fit_zi, by = c("Fluency"), dpar = "zi") +
  scale_x_discrete(
    breaks = c("0", "1"),
    labels = c("Fluent", "Disfluent")
  ) +
  scale_y_continuous(labels = label_percent()) +
  labs(x = "Fluency", y = "Probablity of 0") + 
  theme_publication()

zi_plot

```

## Zero-One-Inflated Beta (ZOIB)

The ZIB model works well if you have 0s in your data, but not 1s.[^7] In our previous example, we nudged our 1s to .99. @kubinec2022 showed that this practice can result in serious distortion of the outcome as the sample size 𝑁 grows larger, resulting in ever smaller values that are “nudged”. Because the Beta distribution is a non-linear model of the outcome, values that are very close to the boundary, such as 0.00001 or 0.99999, will be highly influential outliers. Sometimes it is theoretically useful to model both zeros and ones as separate processes or to consider these values as essentially similar parts of the continuous response, as we show later in the ordered Beta regression model. For example, this is important in visual analog scale data where there might be a prevalence of responses at the bounds [@kong2016], in JOL tasks [@wilford2020], or in a free-list task where individuals provide open responses to some question or topic which are then recoded to fall between 0-1 [@bendixen2023]. Here 0s and 1s are meaningful; 0 means item was not listed and 1 means the item was listed first.

[^7]: In cases where there are large amounts of 1s but no zeros you can fit a one-inflated model. Currently you can do so with the `gamlss` package in R

In our data, we have exactly one value equal to 1. While probably not significant to alter our findings, we can model ones with a special type of model called the zero-one-inflated beta (ZOIB) model if we believe that both 0s and 1s are distinct outcomes.

Similar to our beta and zero-inflated models above, we can fit a ZOIB model in **brms** quite easily using the `zero_one_inflated_beta` family. In this model, we simultaneously estimate the mean ($\mu$) and precision ($\phi$) of the Beta distribution, a zero-one inflation parameter ($\alpha$) that represents the probability that an observation is either exactly 0 or 1, and a conditional one-inflation parameter ($\gamma$) that represents the probability that, given an observation is at one of the endpoints, it is 1. This specification captures the entire range of possible values while remaining constrained between 0 and 1. To get a better sense of how $\alpha$ and $\gamma$ control the distribution of values, @fig-zoib presents simulated data across combinations of these parameters. As $\alpha$ increases, we see a greater proportion of responses at the endpoints. As $\gamma$ increases, the proportion of endpoint responses at 1 grows relative to 0, making the spikes at 1 more prominent as $\gamma$ approaches 1. This visualization illustrates how the ZOIB model flexibly accounts for both the continuous portion of the distribution and the occurrence of exact 0s and 1s.

```{r}
#| label: fig-zoib
#| fig-cap: Simulated data from a ZOIB model illustrating the effects of the zero-one inflation parameter ($\alpha$) and the conditional one-inflation parameter ($\gamma$).
#| fig-width: 8
#| fig-height: 8
# Zero-one inflated beta generator
# Matti V blog post
rzoib <- function(n = 1000, alpha = 0.1, gamma = 0.45, mu = 0.7, phi = 3) {
  a <- mu * phi
  b <- (1 - mu) * phi
  y <- ifelse(
    rbinom(n, 1, alpha),
    rbinom(n, 1, gamma),
    rbeta(n, a, b)
  )
  y
}

# Optimized parameter grid
param_grid <- expand_grid(
  alpha = c(0.05, 0.2, 0.4),
  gamma = c(0.1, 0.5, 0.9),
  mu = 0.7,
  phi = 3
)

# Simulate data
set.seed(123)
sim_data <- param_grid %>%
  mutate(data = pmap(list(alpha, gamma, mu, phi),
                     ~ rzoib(1000, ..1, ..2, ..3, ..4))) %>%
  unnest(data) %>%
  rename(Accuracy = data)

# Classify type
plot_data <- sim_data %>%
  mutate(Type = case_when(
    Accuracy == 0 ~ "Zero",
    Accuracy == 1 ~ "One",
    TRUE ~ "Beta"
  ))

# Plot
p <- ggplot() +
  # Beta histogram
  geom_histogram(
    data = plot_data %>% filter(Type == "Beta"),
    aes(x = Accuracy, y = after_stat(count)),
    bins = 50,
    fill = "gray60",
    color = "white",
    alpha = 0.8
  ) +
  # Spikes at 0 and 1
  geom_col(
    data = plot_data %>%
      filter(Type != "Beta") %>%
      group_by(alpha, gamma, Accuracy, Type) %>%
      summarise(count = n(), .groups = "drop"),
    aes(x = Accuracy, y = count, fill = Type),
    width = 0.01
  ) +
  scale_fill_manual(
    values = c("Zero" = "red", "One" = "blue"),
    name = "Type"
  ) +
  facet_grid(gamma ~ alpha, labeller = label_both, scales = "free_y") +
  labs(
    x = "Accuracy",
    y = "Count"
  ) + 
  theme_publication() + 
    scale_x_continuous(labels = label_percent())


p

ggsave("fig-zoib-plot.png", path=here::here("manuscript", "Figures"))

```

To fit a ZOIB model we use the `bf()` function. We model each parameter as a function of Fluency.

```{r}
#| echo: true

# fit the zoib model

zoib_model <- bf(
  Accuracy ~ Fluency, # The mean of the 0-1 values, or mu
  phi ~ Fluency, # The precision of the 0-1 values, or phi
  zoi ~ Fluency, # The zero-or-one-inflated part, or alpha
  coi ~ Fluency, # The one-inflated part, conditional on the 0s, or gamma
  family = zero_one_inflated_beta()
)
```

We then pass the `zoib_model` to our `brm()` function. The summary of the output is in @tbl-all (under `ZOIB`).

```{r}
#| label: fit zoib model
#| echo: true
#| results: hide
#| message: false

# run the zoib mode using brm

fit_zoib <- brm(
  formula = zoib_model,
  data = fluency_data,
  chains = 4, iter = 2000, warmup = 1000,
  cores = 4, seed = 1234,
  backend = "cmdstanr",
  file = "model_beta_zoib_1"
)
```

```{r}
#| label: tbl zoib
#| echo: false
#| after-caption-space: 0pt
#| eval: false 

zoib_model <- parameters::model_parameters(fit_zoib, "mean")

zoib_model |>
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))), 
  `95% Cr.I` = str_glue("[{CI_low}, {CI_high}]")) |> 
  select(Parameter, Mean, `95% Cr.I`, pd) |> 
  tt(digits = 3) |>
  format_tt(escape=TRUE) 
```

### Model Parameters

The output for the model is pretty lengthy we are estimating four parameters each with their own independent responses and sub-models. All the coefficients are on the logit scale, except $\phi$ , which is on the log scale. Thankfully drawing inferences for all these different parameters, plotting their distributions, and estimating their average marginal effects looks exactly the same—all the `brms` and `marginaleffects` functions we used work the same.

### Predictions and Marginal Effects

With `marginaleffects` we can choose marginalize over all the sub-models, averaged across the 0s, continuous responses, and 1s in the data, or we can model the parameters separately using the `dpar` argument like we did above setting it to: $\mu$, $\phi$, $\alpha$, $\gamma$. Using `avg_predictions()` and not setting `dpar` we can get the predicted probabilities across all the sub-models. We can also plot the overall difference between fluency and disfluency for the whole model with `plot_predictions()`. Our results are very similar to the zero-inflated model from above.

## Ordered Beta Regression

Looking at the output from the ZOIB model, we can see how running a model like this can become vastly complex and computational intensive as it is fitting sub-models for each parameter. The ability to consider 0s and 1s as distinct processes from continuous values comes at a price in terms of complexity. A special version of the ZOIB was recently developed called ordered beta regression [@kubinec2022]. The ordered beta regression model allows for the analysis of continuous data (between 0-1) and discrete outcomes (e.g., 0 or 1) without requiring that either be fully distinct from the other. In the simplest sense, the ordered beta regression model is a hybrid model that estimates a weighted combination of a beta regression model for continuous responses and a logit model for the discrete values of the response.

The weights that average together the two parts of the outcome (i.e., discrete and continuous) are determined by cutpoints that are estimated in conjunction with the data in a similar manner to what is known as an ordered logit model. An in-depth explanation of ordinal regression is beyond the scope of this tutorial [but see @Fullerton2023; @bürkner2019]. At a basic level, ordinal regression models are useful for outcome variables that are categorical in nature and have some inherent ordering (e.g., Likert scale items). To preserve this ordering, ordinal models rely on the cumulative probability distribution. Within an ordinal regression model it is assumed that there is a continuous but unobserved latent variable that determines which of $k$ ordinal responses will be selected. For example on a typical Likert scale from 'Strongly Disagree' to 'Strongly Agree', you could assume that there is a continuous, unobserved variable called 'Agreement'. While we cannot measure Agreement directly, the ordinal response gives us some indication about where participants are on the continuous Agreement scale. $k - 1$ cutoffs are then estimated to indicate the point on the continuous Agreement scale at which your Agreement level is high enough to push you into the next ordinal category (say Agree to Strongly Agree). Coefficients in the model estimate how much different predictors change the estimated *continuous* scale (here, Agreement). Since there’s only one underlying process, there’s only one set of coefficients to work with (proportional odds assumption). In an ordered beta regression, three ordered categories are modeled: (1) exactly zero, (2) somewhere between zero and one, and (3) exactly one. In an ordered beta regression, (1) and (2) are modeled with cumulative logits, where one cutpoint is the the boundary between Exactly 0 and Between 0 and 1 and the other cutpoint is the boundary between *Between 0 and 1* and *Exactly 1.* Somewhere between 0-1 (3) is modeled as a beta regression with parameters reflecting the mean response on the logit scale. Ultimately, employing cutpoints allows for a smooth transition between the bounds and the continuous values, permitting both to be considered together rather than modeled separately as the ZOIB requires.

The ordered beta regression model has shown to be more efficient and less biased than some of the methods discussed [@kubinec2022] herein and has seen increasing usage across the biomedical and social sciences [@wilkesQuantifyingCoextinctionsEcosystem2024; @martinVocalComplexitySocially2024; @smithEcologicalMomentaryAssessment2024; @shresthaBigFiveTraits2024; @nouvianGlyphosateImpairsAversive2023] because it produces only a single set of coefficient estimates in a similar manner to a standard beta regression or OLS.

### Fitting an Ordered Beta Regression

To fit an ordered Beta regression in a Bayesian context we use the `ordbetareg` [@ordbetareg] package. `ordbetareg` is a front-end to the `brms` package that we described earlier; in addition to the functions available in the package, most `brms` functions and plots, including the diverse array of regression modeling options, will work with `ordbetareg` models.

We first load the `ordbetareg` package. You can download it from CRAN or from here: <https://github.com/saudiwin/ordbetareg_pack>.

```{r}
#| echo: true
#| label: load ordbetareg

# load ordbetareg package need to have it installed
library(ordbetareg)
```

The `ordbetareg` package uses `brms` on the front-end so all the arguments we used previously apply here. Instead of the `brm()` function we use `ordbetareg()`.

To fit a model where dispersion does not vary as a function of fluency we can use the below code.

```{r}
#| label: brms ordered beta model
#| results: hide
#| echo: true
# use ordbetareg to fit model
ord_fit_brms <- ordbetareg(Accuracy ~ Fluency,
  data = fluency_data,
  chains = 4,
  iter = 2000,
  backend = "cmdstanr",
  file = "model_beta_ordbeta"
)
```

However, if we want dispersion to vary as a function of fluency we can easily do that. Note the addition of the `phi_reg` argument in `m.phi`. This argument allows us to include a model that explicitly models the dispersion parameter. Because I am modeling $\phi$ as a function of fluency, I set the argument to `both`.

```{r}
#| label: brms ordered beta model phi
#| results: hide
#| echo: true


m.phi <- ordbetareg(bf(Accuracy ~ Fluency, 
                       phi ~ Fluency),
                    data=fluency_data,
                    backend = "cmdstanr",
                     file = "model_beta_ordbeta_phi", 
                    iter = 2000, 
                    cores=4, 
                    phi_reg='both') # log phi
```

## Marginal effects

@tbl-all presents the overall model summary (under `Ordered Beta`). We can use `marginaleffects` to calculate differences on the response scale that average over (or marginalize over) all our parameters.

```{r}
#| label: fit ordbeta

ord_beta_avg <- m.phi |>
  avg_comparisons(
    variables = "Fluency",
    comparison = "difference"
  ) 
```

```{r}
#| label: tbl-ordbeta
#| 
ord_beta_avg |> 
 select(term,contrast, estimate, conf.low, conf.high) |> 
  mutate(
    mutate(mutate(across(where(is.numeric), ~ round(.x, 3)))), 
         `95% Cr.I` = str_glue("[{conf.low}, {conf.high}]")) |>
  rename("Contrast" = "contrast","Mean" = "estimate", "Term" = "term") |>
  select(Term, Contrast, Mean,`95% Cr.I`) |> 
   tt() |>
   format_tt(digits = 3) |>
   format_tt(escape = TRUE)

```

In @tbl-ordbeta the credible interval is close enough to zero relative to its uncertainty that we can conclude there likely aren’t substantial differences between the conditions after taking dispersion and the zeros and ones in our data into account.

### Cutpoints

The model cutpoints are not reported by default in the summary output, but we can access them with the R package `posterior` and the functions `as_draws` and `summary_draws`.

```{r}
#| label: tbl-cutpoints
#| tbl-cap: Cutzero and cutone parameter summary
#| after-caption-space: 0pt
#| echo: false

library(posterior)

as_draws(m.phi, c("cutzero", "cutone")) |>
  summarize_draws() |>
  select(variable, mean, q5, q95) |>
  mutate(across(c(mean, q5, q95), ~ round(.x, 2))) |>
  mutate(`95% Cr.I` = str_glue("[{q5}, {q95}]")) |> 
  rename("Parameter" = "variable", "Mean" = "mean") |> 
  select(Parameter, Mean, `95% Cr.I`) |>
  tt() |>
  format_tt(escape = TRUE)

```

In @tbl-cutpoints, `cutzero` is the first cutpoint (the difference between 0 and continuous values) and `cutone` is the second cutpoint (the difference between the continuous values and 1). These cutpoints are on the logit scale and as such the numbers do not have a simple substantive meaning. In general, as the cutpoints increase in absolute value (away from zero), then the discrete/boundary observations are more distinct from the continuous values. This will happen if there is a clear gap or bunching in the outcome around the bounds. This type of empirical feature of the distribution may be useful to scholars if they want to study differences in how people perceive the ends of the scale versus the middle.

### Ordered Beta Regression Model Fit

The best way to visualize model fit is to plot the full predictive distribution relative to the original outcome. Because ordered beta regression is a mixed discrete/continuous model, a separate plotting function, `pp_check_ordbetareg`, is included in the `ordbetareg` package that accurately handles the unique features of this distribution. The default plot in `brms` will collapse these two features of the outcome together, which will make the fit look worse than it actually is. The `ordbetareg` function returns a list with two plots, `discrete` and `continuous`, which can either be printed and plotted or further modified as `ggplot2` objects (see @fig-ppcheckord).

```{r}
#| label: fig-ppcheckord
#| fig-width: 12
#| fig-height: 8
#| fig-cap: Posterior predictive check for ordered beta regression model. A. Discrete posterior check. B. Continuous posterior check. 

plots <- pp_check_ordbeta(m.phi,
  ndraws = 100,
  outcome_label = "Final Test Accuracy"
) 

plot_grid(plots$discrete, plots$continuous, labels = c("A", "B"))
```

The discrete plot which is a bar graph, shows that the posterior distribution accurately captures the number of different types of responses (discrete or continuous) in the data. For the continuous plot shown as a density plot with one line per posterior draw, the model does a very good job at capturing the distribution.

Overall, it is clear from the posterior distribution plot that the ordered beta model fits the data well. To fully understand model fit, both of these plots need to be inspected as they are conceptually distinct.

#### Model Visualization

`ordbetareg` provides a visualization function called `plot_hess`. This function produces a plot of predicted proportions across the range of our `Fluency` factor. In @fig-heiss we get predicted proportions for Fluency across the bounded scale. Looking at the figure we can see there is much overlap between instructors in the middle portion ($\mu$) . However, we do see some small differences at the zero bounds.

```{r}
#| label: fig-heiss
#| fig-cap: "Heiss Plot of Predicted Probablities across the scale (0-100)"

plot_heiss(ord_fit_brms, grouping_fac="Fluency", recode_group_labels = c("Fluency","Disfluency"))

```

### Ordered Beta Scale

In the `ordbetareg` function there is a `true_bound` argument. In the case where you data in not bounded between 0-1, you can use the argument to specify the bounds of the argument to fit the ordered beta regression. For example, you data might be bounded between 1 and 12. If so, you can model it as such.

# Discussion

The use of Beta regression in psychology, and the social sciences in general, is rare. With this tutorial, we hope to turn the tides. Beta regression models are an attractive alternative to models that impose unrealistic assumptions like normality, linearity, homoscedasticity, and unbounded data. Beyond these models, there are a diverse array of different models that can be used depending on your outcome of interest.

Throughout this tutorial our main aim was to help guide researchers in running analyses with proportional or percentage outcomes using Beta regression and some of it's alternatives. In the current example, we used real data from @wilford2020 and discussed how to fit these models in R, interpret model parameters, extract predicted probabilities and marginal effects, and visualize the results.

Comparing our analysis with that of @wilford2020, we demonstrated that using traditional approaches (e.g., *t*-tests) to analyze accuracy data can lead to inaccurate inferences. Although we successfully reproduced one of their key findings, our use of Beta regression and its extensions revealed important nuances in the results. With a traditional Beta regression model—which accounts for both the mean and the precision (dispersion)—we observed similar effects of instructor fluency on performance. However, the standard Beta model does not accommodate boundary values (i.e., 0s and 1s).

When we applied a zero-inflated Beta regression model, which explicitly accounts for structural zeros, we found no effect of fluency on the mean ($\mu$) part of the model. Instead, the effect of fluency emerged in the structural zero (inflated zero; $\alpha$) component. This pattern was consistent when using a zero-one-inflated Beta (ZOIB) model. Furthermore, we fit an ordered Beta regression model [@kubinec2022], which appropriately models the full range of values, including 0s and 1s. Here, we did not observe a reliable effect of fluency on the mean once we accounted for dispersion.

These analyses emphasize the importance of fitting a model that aligns with the nature of the data. The simplest and recommended approach when dealing with data that contains zeros and/or ones is to fit an ordered Beta model, assuming the process is truly continuous. However, if you believe the process is distinct in nature, a ZIB or ZOIB model might be a better choice. Ultimately, this decision should be guided by theory.

For instance, if we believe fluency influences the structural zero part of the model, we might want to model this process separately using a ZIB or ZOIB. With the current dataset, fluency might affect specific aspects of performance (such as the likelihood of complete failure) rather than general performance levels. This effect could be due to participant disengagement during the disfluent lecture. If students fail to pay attention because of features of disfluency, they may miss relevant information, leading to a floor effect at the test. If this is the case, we would want to model this appropriately. However, if we believe fluency effects general performance levels, a model that takes in to account the entire process accounting for the zeros and ones might be appropriate.

In the discussion section of @wilford2020, they were unable to offer a tenable explanation for performance differences based on instructor fluency. A model that accounts for the excess zeros in the dataset provides one testable explanation: watching a disfluent lecture may lead to lapses in attention, resulting in poorer performance in that group. These lapses, in turn, contribute to the observed differences in the fluent condition. This modeling approach opens a promising avenue for future research—one that would have remained inaccessible otherwise.

Not everyone will be eager to implement the techniques discussed herein. In such cases, the key question becomes: What is the least problematic approach to handling proportional data? One reasonable option is to fit multiple models tailored to the specific characteristics of your data. For example, if your data contain zeros, you might fit two models: a traditional OLS regression excluding the zeros, and a logistic model to account for the zero versus non-zero distinction. If your data contain both zeros and ones, you could fit separate models for the zeros and ones in addition to the OLS model. There are many defensible strategies to choose from depending on the context. However, we do not recommend transforming the values of your data (e.g., 0s to .01 and 1s to .99) or ignoring the properties of your data simply to fit traditional statistical models.

In this tutorial, we demonstrated how to analyze these models from a Bayesian perspective. While we recognize that not everyone identifies as a Bayesian, implementing these models using a Bayesian framework is relatively straightforward—it requires only a single package, lowering the barrier to entry. For those who prefer frequentist analyses, several R packages are available. For standard beta regression, the `betareg` package [@betareg] is a solid option, while more complex models such as zero-inflated and ordered beta regressions can be implemented using `glmmTMB` [@glmmTMB]. For fitting zero-one models, there is a new implementation in @betareg, that allows you to model these types of data.

## Conclusion

Overall, this tutorial emphasizes the importance of modeling the data you have. Although the example provided is relatively simple (a one-factor model with two levels), we hope it demonstrates that even with a basic dataset, there is much nuance in interpretation and inference. Properly modeling your data can lead to deeper insights, far beyond what traditional measures might offer. With the tools introduced in this tutorial, researchers now have the means to analyze their data effectively, uncover patterns, make accurate predictions, and support their findings with robust statistical evidence. By applying these modeling techniques, researchers can improve the validity and reliability of their studies, ultimately leading to more informed decisions and advancements in their respective fields.

## References

<!-- References will auto-populate in the refs div below -->

::: {#refs}
:::
